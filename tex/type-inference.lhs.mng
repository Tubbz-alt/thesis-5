%% -*- mode: LaTeX compile-command: "cd ..; make compile" -*-

%if style == newcode
%include rae.fmt

\begin{code}

import Data.Maybe ( isNothing )
import Data.Kind
import Data.Singletons

\end{code}

%endif

\chapter[Type inference and elaboration]{Type inference and elaboration, or How to \bake/ a \pico/}
\label{cha:type-inference}

\pref{cha:dep-haskell} presents the additions to modern Haskell to make
it Dependent Haskell, and \pref{cha:pico} presents \pico/, the internal
language to which we compile Dependent Haskell programs. This chapter
formally
relates the two languages by defining a type inference/elaboration algorithm,\footnote{I refer to \bake/ variously as an elaboration algorithm, a type inference
algorithm, and a type checking algorithm. This is appropriate, as it is all three.
In general, I do not differentiate between these descriptors.}
\bake/,
checking Dependent Haskell code and producing a well-typed \pico/ program.

At a high level, \bake/ is unsurprising. It simply combines the ideas
of several pieces of prior
work~\cite{outsidein,visible-type-application,gundry-thesis} and targets
\pico/ as its intermediate language. Despite its strong basis in prior work,
\bake/ exhibits a few novelties:
\begin{itemize}
\item Perrhaps its biggest innovation is how
it decides between dependent and non-dependent pattern matching depending
on whether the algorithm is in checking or synthesis mode. (See also
\pref{sec:bidir-dependent-pattern-match}.)
%format mathrms = "[[_s]]"
\item It turns out that checking the annotated expression
|(\ (x :: mathrms) -> ...) :: forall x -> ...| depends on whether
or not the type annotation describes a dependent function. This came
as a surprise. See \pref{sec:annotated-lambdas}.
\item The subsumption relation allows an unmatchable function to be
subsumed by a matchable one. That is, a function expecting an unmatchable
function |a -> b| can also accept a matchable one |a !-> b|.
\end{itemize}

After presenting the elaboration algorithm, I discuss the metatheory
in \pref{sec:bake-metatheory}. This section include a soundness result
that the \pico/ program produced by \bake/ is well-typed. It also relates
\bake/ both to \outsidein/ and the bidirectional type system (``System SB'')
from \citet{visible-type-application}, arguing that \bake/ is a conservative
extension of both.

Full statements of all theorems and definitions, with proofs, appear
in \pref{app:inference}.

\section{Overview}
\label{sec:solv-spec}

\Bake/ is a bidirectional~\cite{local-type-inference}
constraint-generation algorithm~\cite{remy-attapl}. It walks over
the input syntax tree and generates constraints, which are later solved.
It can operate in either a synthesis mode (when the type is unknown)
or in checking mode (when the expected type of an expression is known). Like
prior work~\cite{outsidein,gundry-thesis}, I leave the details of the solver
unspecified; any solver that obeys the properties described in
\pref{sec:solver-properties} will do. In practice, the solver will be the one
currently implemented in GHC. Despite the fact that the dependency tracking
described here is omitted from \citet{outsidein}, the most detailed description of
GHC's solver,\footnote{In the text of the paper describing
  \outsidein/~\cite{outsidein}, the authors separate out the constraint
  generation from the solver. They call the constraint-generation algorithm
  \outsidein/ and the solver remains unnamed. I use the moniker \outsidein/ to
  refer both to the constraint-generation algorithm and the solver.}
the solver as implemented does indeed do dependency tracking and should
support all of the innovations described in this chapter.

Constraints in \bake/ are represented by \emph{unification telescopes}, which
are a list of possibly-dependent unification variables,\footnote{Depending on
  the source, various works in the literature refer to unification variables
  as existential variables (e.g., \cite{simple-bidirectional}) or
  metavariables (e.g., \cite{gundry-thesis} and the GHC source code). I prefer
  unification variables here, as I do not wish to introduce confusion with
  existentials of data constructors nor the metavariables of my developed
  metatheory.} with their types. Naturally, there are two sorts of unification
variables: types $[[au]]$ and coercions $[[cu]]$. The solver finds concrete
types to substitute in for unification variables $[[au]]$ and concrete
coercions to substitute in for unification variables $[[cu]]$. Implication
constraints, a key innovation of \citet{outsidein}, are handled by classifying
unification variables by quantified kinds and propositions. See
\pref{sec:quantified-kinds-and-props}.

The algorithm is stated as several judgments of the following general form:
\[
[[S]];[[P]] [[|->]] \mathit{inputs} [[~>]] \mathit{outputs} [[-|]] [[O]]
\]
Most judgments are parameterized by a fixed signature $[[S]]$ that defines
the datatypes that are in scope.\footnote{I do not consider in this
dissertation how these
signatures are formed. To my knowledge, there is no formal presentation
of the type-checking of datatype declarations, and I consider formalizing
this process and presenting an algorithm to be important future work.}
The context $[[P]]$ is a generalization of contexts $[[G]]$; a context
$[[P]]$ contains both \pico/ variables and unification variables.
Because this is an algorithmic treatment of type inference, the notation
is careful to separate inputs from outputs. Everything to the left of
$[[~>]]$ is an input; everything to the right is an output. Most judgments
also produce an output $[[O]]$, which is a unification telescope, containing
bindings for only unification variables. This takes the place of the emitted
constraints seen in other constraint-generation algorithms. It also serves
as a context in which to type-check future parts of the syntax.

The solver's interface looks like this:
\[
[[S;P |->solv O ~> D ; Z]]
\]
That is, it takes as inputs the current environment and a unification telescope.
It produces outputs of $[[D]]$, a telescope of variables to quantify over,
and $[[Z]]$, the \emph{zonker} (\pref{sec:zonking}),
which is an idempotent
substitution from unification variables to other types/coercions.
To understand the output $[[D]]$, consider checking the declaration
|y = \x -> x|. The variable |x| gets assigned a unification variable type
$[[au]]$. No constraints then get put on that type. When trying to solve
the unification telescope $[[au :Irrel forall{}.Type{}]]$, we have nothing to do.
The answer is, of course, to generalize. So we get $[[D = a :Irrel Type{}]]$
and $[[Z = a/au]]$. In the constraint-generation rules for declarations,
the body of a declaration and its type are generalized over $[[D]]$.
(See \rul{IDecl\_Synthesize} in \pref{sec:idecl}.)

Writing a type inference algorithm for a dependently typed language presents
a challenge in that the type of an expression can be very intricate. Yet
we still wish to infer (simple) types for unannotated expressions. To resolve
this tension, \bake/ adheres to the following:
\theoremstyle{plain}
\newtheorem*{guidingprinciple}{Guiding Principle}
\begin{guidingprinciple}
In absence of other information, infer a simple type.
\end{guidingprinciple}
\begin{guidingprinciple}
Never make a guess.
\end{guidingprinciple}
For example, consider inferring a type for
\begin{code}
compose f g = \x -> f (g x)
\end{code}
The function |compose| could naively be given either of the following types:
\begin{spec}
compose  ::  (b -> c) -> (a -> b) -> (a -> c)
compose  ::  forall  (a :: Type)
                     (b :: a -> Type) 
                     (c :: forall (x :: a) -> b x -> Type)
         .   pi      (f :: forall (x :: a). pi (y :: b x) -> c x y)
                     (g :: pi (x :: a) -> b x)
                     (x :: a)
         ->  c x (g x)
\end{spec}
%if style == newcode
\begin{code}
test4 = compose :: (b -> c) -> (a -> b) -> (a -> c)
{-
dcomp :: forall (a :: Type)
                (b :: a ~> Type)
                (c :: forall (x :: a). Proxy x ~> b @@ x ~> Type)
                (f :: forall (x :: a) (y :: b @@ x). Proxy x ~> Proxy y ~> c @@ (!Proxy :: Proxy x) @@ y)
                (g :: forall (x :: a). Proxy x ~> b @@ x)
                (x :: a).
         Sing f
      -> Sing g
      -> Sing x
      -> c @@ ('Proxy :: Proxy x) @@ (g @@ ('Proxy :: Proxy x))
dcomp f g x = f `applySing` _
-}
\end{code}
%endif
However, we surely want inference to produce the first one. If inference
did not tend toward simple types, there would be no hope of retaining
principal types in the system. I do not prove that \bake/ infers principal
types, as doing so is meaningless without some non-deterministic specification
of the type system, which is beyond the scope of this work. However, I still
wish to design Dependent Haskell with an eye toward establishing a
principal types result in the future. Just like how inferring only rank-1 types
still allows for higher-rank types in a bidirectional type system~\cite{practical-type-inference}, it is my hope that inferring only simple types will
allow for Dependent Haskell to retain principal types.

The second guiding principle is that \bake/ should never make guesses.
Guesses, after all, are sometimes wrong. By ``guess'' here, I mean that
the algorithm and solver should never set the value of a unification variable
unless doing so is the only possible way an expression can be well typed.
Up until this point, GHC's type
inference algorithm has resolutely refused to guess. This decision manifests
itself, among other places, in GHC's inability to work with a function |f :: F a -> F a|, where |F| is a type function.\footnote{Unless |F| is
known to be injective~\cite{injective-type-families}.} The problem is that,
from |f 3|, there is no way to figure out what |a| should be, and GHC will
not guess the answer.

A key consequence of not making any guesses is that \bake/ (more accurately,
the solver it calls) does no higher-order unification. Consider this example:
\begin{spec}
fun :: a -> (f $ a)   -- NB: |f| is not a matchable function

bad :: Bool -> Bool
bad x = fun x
\end{spec}
%if style == newcode
\begin{code}
fun = undefined
type a $ b = a b
\end{code}
%endif
In the body of |bad|, it is fairly clear that we should unify |f| with the
identity function. Yet the solver flatly refuses, because doing so amounts to a
guess, given that there are
many ways to write the identity function.\footnote{Note that my development
does not natively support functional extensionality, so that these different
ways of writing an identity function are not equal to one another.}

In my choice to avoid higher-order unification, my design diverges from the
designs of other dependently typed languages, where higher-order unification
is common. Time will tell whether the predictability gotten from avoiding
guesses is worth the potential annoyance of lacking higher-order unification.
Avoiding guesses is also critical for principal types. See
\citet[Section 3.6.2]{outsidein} for some discussion.

Now that we've seen the overview, let's get down to details.

\section{Haskell grammar}

\begin{figure}
\[
\begin{array}{rcl@@{\quad}l}
[[_t]],[[_k]] &\bnfeq& [[a]] \bnfor [[\ _qvar . _t]] \bnfor [[/\ _qvar. _t]] \bnfor [[_t1 _t2]] \bnfor [[_t1 @_t2]] \bnfor [[_t :: _s]] & \text{type/kind} \\
&\bnfor& [[case _t of _alts]] \bnfor [[_t1 -> _t2]] \bnfor [[_t1 '-> _t2]] \bnfor [[fix _t]]  \\
&\bnfor& [[let x := _t1 in _t2]] \\
[[_qvar]] &\bnfeq& [[_aqvar]] \bnfor [[@_aqvar]] & \text{quantified variable} \\
[[_aqvar]] &\bnfeq& [[a]] \bnfor [[a :: _s]] & \text{quantified variable (w/o vis.)} \\
[[_alt]] &\bnfeq& [[_pat -> _t]] & \text{case alternative} \\
[[_pat]] &\bnfeq& [[H xs]] \bnfor [[_]] & \text{pattern} \\
[[_s]] &\bnfeq& [[quant _qvar. _s]] \bnfor [[_t => _s]] \bnfor [[_t]] & \text{type scheme/polytype} \\
[[quant]] &\bnfeq& [[forall]] \bnfor [['forall]] \bnfor [[pie]] \bnfor [['pie]] & \text{quantifier} \\[1ex]
[[_decl]] &\bnfeq& [[x :: _s := _t]] \bnfor [[x := _t]] & \text{declaration} \\
[[_prog]] &\bnfeq& [[empty]] \bnfor [[_decl; _prog]] & \text{program}
\end{array}
\]
\caption{Formalized subset of Dependent Haskell}
\label{fig:formal-haskell-grammar}
\end{figure}

I must formalize a slice of Dependent Haskell in order to describe an
elaboration procedure over it. The subset of Haskell I will consider is
presented in \pref{fig:formal-haskell-grammar}. Note that all Haskell
constructs are typeset in upright Latin letters; this is to distinguish
these from \pico/ constructs, typeset in italics and often using Greek
letters.

The version of Dependent Haskell presented here differs in a few details
from the language presented in \pref{cha:dep-haskell}. These differences
are to enable an easier specification of the elaboration algorithm. Translating
between the ``real'' Dependent Haskell of \pref{cha:dep-haskell} and this
version can be done by a preprocessing step. Critically,
(but with one exception)
no part of
this preprocessor needs type information. For example,
|forall a b. ...| is translated to $[[forall @&&ax. forall @&&bx. ...]]$
so that it is easier to consider individual bound variables.

The exception to the irrelevance of type information is in dealing with
pattern matches. Haskell pattern matches can be nested, support guards,
perhaps view patterns~\cite{view-patterns},
perhaps pattern synonyms~\cite{pattern-synonyms}, etc. However, translating
such a rich pattern syntax into a simple one
is a well-studied problem with widely-used
solutions~\cite{augustsson-compiling-pattern-matching,wadler-pattern-matching}
and I thus consider the algorithm as part of the preprocessor and do 
not consider this further.

\subsection{Dependent Haskell modalities}

Let's now review some of the more unusual annotations in Dependent Haskell,
originally presented in \pref{cha:dep-haskell}. Each labeled paragraph
 below describes
an orthogonal feature (visibility, matchability, relevance).

\paragraph{The $\at$ prefix}
Dependent Haskell uses an $\at$ prefix to denote an argument that would
normally be invisible. It is used in two places in the grammar:
\begin{itemize}
\item An $\at$-sign before an argument indicates that the argument may
be omitted and left to inference. This follows the treatment in my prior
work on invisible arguments~\cite{visible-type-application}.
\item An $\at$-sign before a quantified variable (in the definition for
$[[_qvar]]$) indicates that the actual argument may be omitted when
calling a function. In a $\lambda$-expression, this would indicate
a pattern that matches against an invisible argument
(\pref{sec:visible-type-pat}). In a $\Pi$- or $\forall$-expression, the
$\at$-sign is produced by the preprocessor when it encounters a
|forall ... .| or |pi ... .| quantification.
\end{itemize}

\paragraph{Ticked quantifiers}
Three of the quantifiers that can be written in Dependent Haskell
come in two varieties: ticked and unticked. An ticked quantifier
serves in the type of matchable (that is, generative and injective)
functions, whereas the unticked quantifier describes an unrestricted
function space. Recall that type constructors and data constructors
are typed by matchable functions, whereas ordinary $\lambda$-expressions
are not.

\paragraph{Relevance}
The difference between |forall| and |pi| in Dependent Haskell is that
the former defines an irrelevant abstraction (fully erased during compilation)
while the latter describes a relevant abstraction (retained at runtime).
In terms, an expression introduced by $\lambda$ is a relevant abstraction;
one introdcued by $\Lambda$ is an irrelevant one.

\subsection{Omissions from the Haskell grammar}

There are two notable omissions from the grammar in \pref{fig:formal-haskell-grammar}.

\paragraph{Type constants} 
The Haskell grammar contains no
production for $[[H]]$, a type constant. This is chiefly because
type constants must be saturated with respect to universals in \pico/,
whereas we do not need this restriction in Haskell. Accordingly, type
constants are considered variables that expand to type constants
that have been $\eta$-expanded to take their universal arguments in
a curried fashion.

\paragraph{Recursive |let|}
Following the decision not to include a \keyword{letrec} construct in
\pico/ (\pref{sec:no-letrec}),
the construct is omitted from the formalized subset of Haskell as well.
Having a formal treatment of \keyword{letrec} would require a formalization
of Haskell's consideration of polymorphic recursion~\cite{meertens-polymorphic-recursion,mycroft-polymorphic-recursion,henglein-polymorphic-recursion}, whereby
definitions with type signatures can participate in polymorphic recursion while
other definitions cannot. In turn, this would require a construct where
a polymorphic function is treated monomorphically in a certain scope and
polymorphically beyond that scope.\footnote{Readers familiar with the
internals of GHC may recognize its |AbsBinds| data constructor in this
description. Formalizing all of its intricacies would indeed be required
to infer the type of a \keyword{letrec}.} The problems faced here are not
unique to (nor made particularly worse by) dependent types. I thus have
chosen to exclude this construct for simplicity.

\paragraph{}
We have now reviewed the source language of \bake/, and the previous
chapter described its target language, \pico/. I'll now fill in the gap
by introducing the additions to the grammar needed to describe the inference
algorithm.

\section{Unification variables}

\begin{figure}
Metavariables:
\[
\begin{array}{rl@@{\qquad}rl}
[[au,bu]] & \text{unification type variable} & [[cu]] & \text{unification coercion variable}
\end{array}
\]
Grammar extensions:
\[
\begin{array}{rcl@@{\quad}l}
[[t]] &\bnfeq& \ldots \bnfor [[au _ ps]] & \text{type/kind} \\
[[g]] &\bnfeq& \ldots \bnfor [[cu _ ps]] & \text{coercion} \\[1ex]
[[zu]] &\bnfeq& [[au]] \bnfor [[cu]] & \text{unification variable} \\
[[Z]] &\bnfeq& [[empty]] \bnfor [[Z, forall zs. t / au]] \bnfor [[Z, forall zs. g/cu]] & \text{zonker (\pref{sec:zonker})} \\
[[X]] &\bnfeq& [[empty]] \bnfor [[X, zu |-> ps]] & \text{generalizer (\pref{sec:generalizer})} \\[1ex]
%
[[u]] &\bnfeq& [[au :rel forall D. k]] \bnfor [[cu : forall D. phi]] & \text{unif.~var.~binding} \\
[[O]] &\bnfeq& [[empty]] \bnfor [[O, u]] & \text{unification telescope} \\
[[P]] &\bnfeq& [[empty]] \bnfor [[P, d]] \bnfor [[P, u]] & \text{typing context} \\%[1ex]
%[[J]] &\bnfeq& \multicolumn{2}{l}{\text{stand-in for an arbitrary judgment (\pref{sec:J})}}
\end{array}
\]
I elide the $[[forall]]$ when the list of variables or telescope quantified
over would be empty.
\caption{Additions to the grammar to support \bake/.}
\label{fig:bake-grammar}
\end{figure}

The extensions to the grammar to support inference are in \pref{fig:bake-grammar}.
These extensions all revolve around supporting unification variables, which
are rather involved. One might think that unification variables need not be
so different from ordinary variables; constraint generation could produce a
telescope of these unification variables and solving simply produces a
substitution. However, this naive view does not work out
because of unification variable generalization.\footnote{The treatment
of unification variables throughout \bake/ is essentially identical to
the treatement by \citet{gundry-thesis}.}

Consider a $\lambda$-abstraction over the variable $[[x]]$. When doing
constraint generation inside of the $\lambda$, the kinds of fresh unification
variables might mention $[[x]]$. Here is a case in point, which will serve
as a running example:
%
\begin{spec}
poly :: forall k (a :: k) -> ...

example = \ k a -> poly k a
\end{spec}
%
Type inference can easily discover that the kind of |a| is |k|. But in order
for the inference algorithm to do this, it must be aware that |k| is in
scope before |a| is. Note that when we call the solver after type-checking
the entire body of |example|, |k| is \emph{not} in scope. Thus, as we produce
the unification telescope during constraint-generation over the body of
|example|, we must somehow note that the unification variable $[[au]]$
(the type of |a|) can mention |k|.

This means that unification variable bindings are quantified over a
telescope $[[D]]$. (You can see this in the definition for $[[u]]$
in \pref{fig:bake-grammar}.) In the vocabulary of \outsidein/, the
bindings in $[[D]]$ are the \emph{givens} under which a unification
variable should be solved for.

\subsection{Zonking}
\label{sec:zonking}
Solving produces a substitution from unification variables to types/coercions.
Following the nomenclature within GHC, I call applying this substitution
\emph{zonking}. The substitution itself, written $[[Z]]$, is called a
\emph{zonker}.

Zonkers pose a naming problem. Consider solving
to produce the zonker for |example|, above. Suppose the type of |a| is
assigned to be $[[au]]$. We would like to zonk $[[au]]$ to |k|.
However, as before, |k| is out of scope when solving for $[[au]]$. We thus
cannot just write $|k|/[[au]]$, as that would violate the Barendregt
convention, where we can never name a variable that is out of scope
(as it might arbitrarily change due to $\alpha$-renaming).

The solution to this problem is to have all occurrences of unification
variables applied to vectors $[[ps]]$.\footnote{Recall that $[[p]]$ is
a metavariable that can stand for either a type or a coercion. Thus
$[[ps]]$ is a mixed list of types and coercions, suitable for substituting
in for a list of type/coercion variables $[[zs]]$.} When we zonk a
unification variable occurrence $[[au _ ps]]$, the vector $[[ps]]$ is
substituted for the variables in the telescope $[[D]]$ that $[[au]]$'s kind
is quantified over.

Here is the formal definition of zonking:

\begin{definition*}[Zonking {[\pref{defn:zonking}]}]
A zonker can be used as a postfix function. It operates homomorphically
on all recursive forms and as the identity operation on leaves other
than unification variables. Zonking unification variables
is defined by these equations:
\[
\begin{array}{r@@{\quad}c@@{\quad}r@@{\;}l}
[[forall zs. t/au \in Z]] & \implies & [[au_ps[Z] &= t[ps[Z]/zs ] ]] \\
\text{otherwise} && [[au_ps[Z] &= au_{{ps[Z]}} ]] \\[1ex]
[[forall zs. g/cu \in Z]] & \implies & [[cu_ps[Z] &= g[ps[Z]/zs ] ]] \\
\text{otherwise} && [[cu_ps[Z] &= cu_{{ps[Z]}} ]]
\end{array}
\]
\end{definition*}

In our example, we would say that |a| has the type $[[au_&&kx]]$, where
$[[au :Irrel forall &&kx :Irrel Type{}. Type{}]]$. The solver will create
a zonker with the mapping $[[forall &&jx. &&jx/au]]$ (where I have
changed the variable name for demonstration). This will zonk
$[[au_&&kx]]$ to become $[[&&jx[&&kx/&&jx] ]]$ which is, of course
$[[&&kx]]$ as desired.

Note that the quantification we see here is very different from normal
$\Pi$-quantification in \pico/. These quantifications are fully second-class
and may be viewed almost as suspended substitutions.

\subsection{Additions to \pico/ judgments}

\begin{figure}
\ottdefnUTy{}
\ottdefnUCo{}
\ottdefnUCtx{}
\caption{Extra rules in \pico/ judgments to support unification variables}
\label{fig:bake-pico-judgments}
\end{figure}

The validity and typing judgments in \pico/ all work over signatures
$[[S]]$ and contexts $[[G]]$. In \bake/, however, we need to be able
to express these judgments in an environment where unification variables
are in scope. I thus introduce mixed contexts $[[P]]$, containing both
\pico/ variables and unification variables.

Accordingly, I must redefine all of the \pico/ judgments to support
unification variables in the context. These judgments are written with
a $[[|=]]$ turnstile in place of \pico/'s $[[|-]]$ turnstile. There are
also several new rules that must be added to support unification variables.
These rules appear in \pref{fig:bake-pico-judgments}.

Note the rules \rul{Ty\_UVar} and \rul{Co\_UVar} that support unification
variable occurrences. The unification variables are applied to vectors
$[[ps]]$ which must match the telescope $[[D]]$ in the classifier for
the unification variable. In addition, this vector is substituted directly
into the unification variable's kind.

These definitions support all of the properties proved about the original
\pico/ judgments, such as substitution and regularity. The statements
and proofs are in \pref{app:inference}.

\subsection{Untouchable unification variables}
\label{sec:untouchability}

\citet[Section 5.2]{outsidein} introduces the notion of \emph{touchable}
unification variables, as distinct from \emph{untouchable} variables. Their
observation is that it is harmful to assign a value to a ``global'' unification
variable when an equality constraint is in scope. ``Global'' here means that
the unification variable has a larger scope than the equality constraint.
We call the ``local'' unification 
variables touchable, and the ``global'' ones untouchable. \outsidein/ must
manually keep track of touchability; the set of touchable unification variables
is an extra input to its solving judgment.

In \bake/, on the other hand, tracking touchability is very easy with its
use of unification telescopes: all unification variables quantified by the
same equality constraints as the constraint under consideration are touchable;
the rest are untouchable.

To make this all concrete, let's look at a concrete example (taken from
\citet{outsidein}) where the notion of touchable variables is beneficial.

Suppose we have this definition:
%
\begin{code}
data T a where
  K :: (Bool ~ a) => Maybe Int -> T a
\end{code}
%
I have written this GADT with an explicit equality constraint in order to
make the use of this constraint clearer. The definition for |K| is entirely
equivalent to saying |K :: Maybe Int -> T Bool|.

%{
%format bu = "\beta"
%format bu1
We now wish to infer the type of
\[
|\x -> case x of { K n -> isNothing n }|
\]
where |isNothing :: forall a. Maybe a -> Bool| checks for an empty |Maybe|.
Consider any
mention of a new unification variable to be fresh. We assign |x| to have type
$[[au0]]$ and the result of the function to have type $[[bu0]]$. By the
existence of the constructor |K| in the |case|-match, we learn that $[[au0]]$
should really be $[[&&Tx{} au1_{}]]$. Inside the |case| alternative, we
now have a given constraint $[[&Bool{} [Type{}]~[Type{}] au1_{}]]$. We then
instantiate the polymorphic |isNothing| with a unification variable $[[bu1]]$,
so that the type of |isNothing| is |Maybe bu1 -> Bool|. We can now emit two equality
constraints:
\begin{itemize}
\item The argument type to |isNothing| (|Maybe bu1|) must match the type of |n| (|[Int]|).
\item The return
type of the |case| expression ($[[bu0]]$) is the return type of |isNothing| (|Bool|).
\end{itemize}
Pulling this all together, we get the following unification telescope:
\[
[[O]] =[
\begin{array}[t]{l}
[[au0 :Irrel forall{}. Type{}]],\\
[[bu0 :Irrel forall{}. Type{}]],\\
[[au1 :Irrel forall{}. Type{}]],\\
[[cu0 : forall{}. au0_{} [Type{}]~[Type{}] &&Tx{} au1_{}]],\\
[[bu1 :Irrel forall c : &Bool{} [Type{}]~[Type{}] au1_{}. Type{}]], \\
[[cu1 : forall c : &Bool{} [Type{}]~[Type{}] au1_{}. (&Maybe{} bu1_c [Type{}]~[Type{}] &Maybe{} &Int{})]], \\
[[cu2 : forall c : &Bool{} [Type{}]~[Type{}] au1_{}. (bu0_{} [Type{}]~[Type{}] &Bool{})]]
\end{array}
\]

Before we walk through what the solver does with such a telescope, what
\emph{should} it do? That is, what's the type of our original expression?
It turns out that this is not an easy question to answer! That expression
has no principal type. Both of the following are true:
\[
\begin{array}{l}
| (\x -> case x of { K n -> isNothing n }) :: forall a. T a -> a | \\
| (\x -> case x of { K n -> isNothing n }) :: forall a. T a -> Bool |
\end{array}
\]
%
%if style == newcode
\begin{code}
test1 = (\x -> case x of { K n -> isNothing n }) :: forall a. T a -> a
test2 = (\x -> case x of { K n -> isNothing n }) :: forall a. T a -> Bool
-- test3 = (\x -> case x of { K n -> isNothing n })
\end{code}
%endif
%
Note that neither |T a -> a| nor |T a -> Bool| is more general than the
other.

We would thus like the solver to fail when presented with this unification
telescope. This is true, even though there is a solution to the inference
problem (that is, a valid zonker $[[Z]]$ with a telescope of quantified
variables $[[D]]$; see the specification of $[[|->solv]]$, \pref{sec:solv-spec}):
\begin{align*}
%\begin{array}{rcl}
[[D]] &= [[&&ax :Irrel Type{}]] \\
[[Z]] &=
\begin{array}[t]{@@{}l}
[[forall{}. &&Tx{} &&ax/au0]], \\
[[forall{}. &Bool{}/bu0]], \\
[[forall{}. &&ax/au1]], \\
[[forall{}. <&&Tx{} &&ax>/cu0]], \\
[[forall c. &Int{}/bu1]], \\
[[forall c. <&Maybe{} &Int{}>/cu1]], \\
[[forall c. <&Bool{}>/cu2]]
\end{array}
%\end{array}
\end{align*}

The problem is that here is another valid substitution for $[[bu0]]$ and $[[cu2]]$:
\[
[[Z]] =
\begin{array}[t]{@@{}l}
 \ldots, \\
[[forall {}. &&ax/bu0]], \\
\ldots, \\
[[forall c. sym c/cu2]]
\end{array}
\]
%
These zonkers correspond to the overall type |T a -> Bool| and |T a -> a|, respectively.

We must thus ensure that the solver rejects $[[O]]$ outright. This is achieved
by making $[[bu0]]$ untouchable when considering solving the $[[cu2]]$ constraint.\footnote{Why this particular mechanism works is discussed in some depth
by \citet[Section 5.2]{outsidein}.}
As described by \citet[Section 5.5]{outsidein}, the solver considers the
constraints individually. When simplifying (\outsidein/'s terminology
for solving a simple, non-implication constraint) the $[[cu1]]$ and
$[[cu2]]$ constraints, any unification variable not quantified by $[[c]]$ is considered
untouchable.\footnote{To make this a bit more formal, I would need to label
the quantification by $[[c]]$ by some label drawn from an enumerable set
of labels. The touchable unification variables would be those quantified
by the same label as the constraint being simplified. We cannot just use
the name $[[c]]$, as names are fickle due to potential $\alpha$-variation.}
Thus, $[[bu0]]$ is untouchable when simplifying $[[cu2]]$, so the solver
will never set $[[bu0]]$ to anything at all. It will remain an ambiguous
variable and a type error will be issued. 

Contrast this with $[[au1]]$, which is also not set by the solver.
This variable, however, is fully unconstrained and can be quantified over
and turned into the non-unification variable $[[&&ax]]$.
There is no way to quantify over $[[bu0]]$, however.

Despite not setting $[[bu0]]$, the solver is free to set $[[bu1]]$ which
is considered touchable, as it is also quantified by $[[c]]$. The unification
variable $[[bu1]]$ is fully local to the |case| alternative body, and setting
it can have no effect outside of the |case| expression. In the terminology
of \outsidein/, that unification would be introduced by $\exists [[bu1]]$
in an implication constraint. In our example, the ability to set $[[bu1]]$
means that we get only one type error reported, not two.
%}

\section{Bidirectional typechecking}

Like previous algorithms for
GHC/Haskell~\cite{practical-type-inference,visible-type-application,gundry-thesis}, \bake/
takes a bidirectional approach~\cite{local-type-inference}. The fundamantal
idea to bidirectional typechecking is that, sometimes, the type inference
algorithm knows what type to look for. When this happens, the algorithm
should take advantage of this knowledge.

Bidirectional typechecking works by defining two separate algorithms: a type
synthesis algorithm and a type checking algorithm. The former is used when
we have no information about the type of an expression, and the latter is
used when we do indeed know an expression's expected type. The algorithms
are mutually recursive because of function applications: knowing the result
type of a function call does not tell you about the type of the function
(meaning the checking algorithm must use synthesis on the function),
but once we know the function's type, we know the type of its arguments
(allowing the synthesis algorithm to use the more informative checking
algorithm).

Historically, bidirectional typechecking in Haskell has been most useful when
considering higher-rank polymorphism---for example, in a type
like |(forall a. a -> a) -> Int|. Motivating higher-rank types would bring
us too far afield, but the literature has helpful examples~\cite{practical-type-inference,visible-type-application} and there is a brief introduction in
\pref{sec:higher-rank-types}. Naturally, Dependent Haskell continues to
use bidirectional typechecking to allow for higher-rank types, but there
is now even more motivation for bidirectionality.

As discussed above (\pref{sec:untouchability}), bringing equality constraints
into scope makes some unification variables untouchable. In practice, this
means that the result type of a GADT pattern match must be known; programmers
must put type annotations on functions that perform a GADT pattern match.

In a dependently-typed language, however, \emph{any} pattern match might
bring equality constraints into scope, where the equality relates the scrutinee
with the pattern. For example, if I say something as simple as
|case b of { True -> x ; False -> y }|, I may want to use the fact that
|b ~ True| when typechecking |x| or |b ~ False| when typechecking |y|. This
is, of course, dependent pattern matching (\pref{sec:dependent-pattern-matching}).
Our problem now is that it seems that \emph{every} pattern match introduces
an equality constraint, meaning that the basic type inference of Haskell might
no longer work.

The solution is to take advantage of the equality available by dependent
pattern matching only when the result type of the |case| expression is
being propagated downwards---that is, when the inference algorithm is
in checking mode. If we do not know a |case| expression's overall type,
then the pattern match is treated as a traditional, non-dependent pattern
match. Without bidirectional typechecking, the user might have to annotate
which kind of match is intended.\footnote{The Dependent Haskell described
by \citet{gundry-thesis} indeed has the user annotate this choice for
|case| expressions. Due to Gundry's restrictions on the availability of
terms in types (see his Section 6.2.3), however, the bidirectional approach would
have been inappropriate in his design.}

\subsection{Invisibility}

As discussed in \pref{sec:dep-haskell-vis}, Dependent Haskell programmers can
choose the visibility of their arguments: A visible argument must be provided
at every function call, while an invisible one may be elided. If the programmer
wants to write an explicit value to use for an invisible argument, prefixing
the argument with $\at$ allows it to stand for the invisible parameter.

In the context of type inference, though, we must be careful. As explored
in my prior work~\cite{visible-type-application}, invisible arguments are
sometimes introduced at the whim of the compiler. For example, consider
%
\begin{code}
-- |isShorterThan :: [a] -> [b] -> Bool|
isShorterThan xs ys = length xs < length ys
\end{code}
%
Note that the type signature is commented out. The function |isShorterThan|
takes two invisible arguments, |a|, and |b|. Which order should they appear
in? Without the type signature for guidance, it is, in general, impossible
to predict what order these will be generalized. See \citet[Section 3.1]{visible-type-application} for more discussion on this point.

Despite the existence of functions like |isShorterThan| with fully-inferred
type signatures, we wish to retain principal types in our type system---at
least in the subset of the language that does not work with equality
constraints.
We thus must have \emph{three} different levels of visibility:
\begin{description}
\item[Required] parameters (also called visible) must be provided at
function call sites.
\item[Specified] parameters are invisible, but their order is user-controlled.
These parameters are to functions with type signatures or with an explicit
|forall ...|.
\item[Inferred] parameters (called ``generalized'' in \citet{visible-type-application}) are ones invented by the type inference algorithm (like the parameter
|a| in the example used to explaim untouchabile variables;
see \pref{sec:untouchability}). They cannot ever be instantiated
explicitly. All coercion abstractions are inferred.
\end{description}
Note that these three levels of visibility are not a consequence of dependent
types, but of having an invisibility override mechanism; these three levels
of visibility are fully present in GHC 8. In the judgments that form \bake/,
I often write a subscript $[[Req]]$, $[[Spec]]$, or $[[Inf]]$to $\Pi$ symbols indicating the visibility of the
binders quantified over. These subscripts have no effect on well-formedness
of types and are completely absent from pure \pico/.

Following my prior work, both the synthesis
and checking algorithms are split into two judgments apiece: one written
$[[|->ty]]$ and one written $[[*|->ty]]$. The distinction is that the
latter works with types that may have invisible binders, while the former
does not. For example, a type produced by the $[[|->ty]]$ judgment in synthesis
mode is guaranteed not to have any invisible (that is, specified or
inferred) binders at the outermost level. Thus when synthesizing the type
of $[[_t1]]$ in the expression $[[_t1 _t2]]$, we use the $[[|->ty]]$ judgment,
as we want any invisible arguments to be inferred in preparation of applying
$[[_t1]]$ to $[[_t2]]$. Considering the algorithm in checking mode,
when processing a traditional $\lambda$-expression, we want the rule to be part of the $[[|->ty]]$ judgment,
to be sure that the algorithm has already skolemized (\pref{sec:skolemization})
the known type down to
one that accepts a visible argument. Conversely, the rule for an expression
like |\ ^^ (at a) -> ...| must belong in the $[[*|->ty]]$ judgment, as we want
to see the invisible binders in the type to match against the invisible argument
the programmer wishes to bind.

The interplay between the starred judgments and the unstarred nudges this
system toward principal types. Having these two different judgments is indeed
one of the main innovations in my prior work~\cite{visible-type-application},
where the separation is necessary to have principal types.

\subsection{Subsumption}
\label{sec:subsumption}

Certain expression forms do not allow inward propagation of a type. As mentioned
above, if we are checking an expression |f x| against a type |tau|, we have
no way of usefully propagating information about |tau| into |f| or |x|.
Instead, we use the synthesis judgment for |f| and then check |x|'s type
against the argument type found for |f|. After all of this, we will get
a type |tau'| for |f x|. We then must check |tau'| against |tau|---but
they do not have to match exactly. For example, if |tau'| is |foall a. a -> a|
and |tau| is |Int -> Int|, then we're fine, as any expression of the former
type can be used at the latter.

What we need here is a notion of \emph{subsumption}, whereby we say that
|forall a. a -> a| \emph{subsumes} |Int -> Int|, written
\[
|forall a. a -> a| [[<=]] |Int -> Int|
\]
For reasons well articulated in prior work~\cite[Section 4.6]{practical-type-inference}, my choice for the subsumption relation does \emph{deep skolemization}.
This means that the types |forall a. Int -> a -> a| and |Int -> forall a. a -> a|
are fully equivalent and is backward compatible with the current treatment
of non-prenex types in GHC.

\begin{figure}
\ottdefnIPrenexSimp{}\\
\ottdefnISubTwoSimp{}\\
\ottdefnISubSimp{}
\caption{Subsumption in \bake/ (simplified)}
\label{fig:subsumption}
\end{figure}

\bake/'s subsumption relation is in \pref{fig:subsumption}. The rules
in this figure are simplified from the full rules (which appear in
\pref{app:subsumption}), omitting constraint generation and elaboration.
The rules in each judgment are meant to be understood as an algorithm,
trying earlier rules before later ones. Thus, for example, rule \rul{Sub\_Unify}
is not as universal as it appears.

The entry point is the bottom, unstarred subsumption judgment. It
computes the prenex form of $[[k2]]$ using the auxiliary judgment
$[[|->pre]]$ and instantiates $[[k1]]$. (The $[[Spec]]$ superscript to
$[[|->inst]]$ says to instantiate any argument that is no more visible
than $[[Spec]]$---that is, either $[[Inf]]$ or $[[Spec]]$ arguments.)
The instantiated $[[k1']]$ and prenexed $[[k2']]$ are then compared using
the starred subsumption judgment.\footnote{The stars on these judgments
have a different meaning than the star on $[[|->ty]]$; they are borrowed
from the notation by \citet{practical-type-inference},
not \citet{visible-type-application}.}

The starred judgment has the
usual contravariance rule for functions. This rule, however, has three
interesting characteristics.
\paragraph{Dependency}
We cannot simply compare $[[k2]] [[<=]] [[k4]]$. The problem is that
$[[k2]]$ has a variable $[[a]]$ of type $[[k1]]$ in scope, whereas
$[[k4]]$ has a variable $[[b]]$ of type $[[k3]]$ in scope. Contrast this
rule to a rule for non-dependent functions where no such bother arises.
In the fully detailed versions of these judgments, learning that
$[[k1 <= k2]]$ gives us a term $[[t]]$ such that
$[[t]] : [[UPI _:Rel k1. k2]]$---that is, a way of converting a $[[k1]]$ into
a $[[k2]]$. I include such a $[[t]]$ when checking whether $[[k3 <= k1]]$.
This $[[t]]$ is then used to convert $[[b]] : [[k3]]$ into a value of type
$[[k1]]$, suitable for substitution in for $[[a]]$. With this substitution
completed, we can perform the subsumption comparison against $[[k4]]$ as
desired.

\paragraph{Matchable functions subsume unmatchable ones}
Rule \rul{Sub\_Fun} includes a subsumptive relationship among
the two flavors of $\Pi$. Whenever an unmatchable $[[UPI]]$-type is expected,
surely a matchable $[[MPI]]$-type will do. Thus we allow either $\Pi$
on the left of the $[[<=]]$. Note that the other way would be wrong: not only
might an unmatchable $[[UPI]]$-type not work where a matchable $[[MPI]]$-type
is expcted, but we also have no way of creating the $[[MPI]]$-type during
elaboration. Our need to elaborate correctly keeps us from getting this
wrong.

\paragraph{$[[Irrel]]$ subsumes $[[Rel]]$}
Finally, the rule also includes a subsumptive relationship among
relevances. If the relevances $[[rel1]]$ and $[[rel2]]$ match up, then
all is well. But also if $[[rel1]]$ is $[[Irrel]]$ and $[[rel2]]$ is $[[Rel]]$,
we are OK. If $[[rel2]]$ is $[[Rel]]$, that says that the expression we
are checking is allowed to use its argument relevantly, but nothing goes
wrong if the expression, in fact, does not (that is, if $[[rel1]]$ is
$[[Irrel]]$). Once again, elaboration keeps us honest here; if the rule
is written
the wrong way around, there is no sound way to elaborate.

\subsection{Skolemization}
\label{sec:skolemization}

In checking mode, the $[[*|->ty]]$ judgment \emph{skolemizes} any invisible
quantifiers in the known type.\footnote{I am following \citet{practical-type-inference} in my use of the word ``skolem''. I understand that this word may
have slightly different connotation in a logical context, but my use here
has become standard in the study of GHC/Haskell.}
As an example, consider
\[
|(\ x -> x) :: forall a. a -> a|
\]
When checking the $\lambda$-expression against that type, we first must
dispose of the |forall a|. This is done by essentially making |a| a 
fresh type constant, equal to no other. This act is called skolemization;
|a| becomes a skolem. The variable |x| is then given
this type |a|, and the body of the $\lambda$ indeed has type |a| as desired.

As we look at more complicated examples, a question arises about how deeply
to skolemize. Here is an illustrative example, taken from prior work~\cite{visible-type-application-extended}:
\begin{code}
x = \ 5 z -> z
 -- |x| is inferred to have type |forall a. Int -> a -> a|

y :: Int -> forall b. b -> b
y = x
\end{code}
In this example, we are checking |x| of type |forall a. Int -> a -> a|
against the type |Int -> forall b. b -> b|. We must be a bit careful here,
though: |x|'s type is fully inferred, and thus its quantification over |a|
is $[[Inf]]$, not $[[Spec]]$. With the right flags,\footnote{\texttt{-fprint-explicit-foralls}, specifically} GHC prints |x|'s type
as |forall {a}. Int -> a -> a| to denote that it is not available for a
visibility override.

The type we are checking against
does not have any invisible binders at the top (its first binder is the
visible one for |Int|), so we do not skolemize. Instead, we look up |x|'s
type and instantiate |a| with a unification variable $[[au]]$. We then
check whether |Int -> alpha -> alpha| subsumes
|Int -> forall b. b -> b|. The subsumption check does deep skolemization
and then compares |Int -> alpha -> alpha| with |Int -> b -> b|. Of course,
the solution is $[[&&bx/au]]$. Yet, there is a lurking problem: $[[au]]$
is added to the unification telescope \emph{before} |b| is skolemized. This
means that it would be ill-scoped for $[[au]]$ to refer to |b|.\footnote{Saying
that this example fails because of scoping is a vast improvement over
the state of affairs in \citet{visible-type-app-extended}, where a delicate
line of reasoning based on the subtleties of the Barendregt convention is
necessary to show how this example goes awry. By tracking our unification
variables in a telescope, problems like this become much clearer.} However,
it is quite unfortunate that this example is rejected, because the subsumption
judgment, with its deep skolemization, would have this work out if only we
didn't instantiate that $[[Inf]]$ binder so eagerly.

The solution to this problem proposed in prior work is to do deep skolemization
in the checking $[[*|->ty]]$ judgment. This works in the System SB
of \citet{visible-type-application}. However, it fails us here. The problem
is that Dependent Haskell allows for constructs like
|\ n (at a) -> ...|. If we check that expression against
|Int -> forall a. a -> a|, we want the |a|s to match up. Yet deeply skolemizing
the type we are checking against will eliminate the |a| and our algorithm
will reject the code. We thus instead do shallow skolemization in
$[[*|->ty]]$ and instead generalize the synthesized type right before
the subsumption check. This alternative solution to the problem was
also proposed in my prior work but rejected because of mild implementation
challenges. Now with a clear motivation for shallow skolemization over
deep, that is what we will have to do. I do not expect this implementation
burden to be the leading factor in the challenge of
implementing the ideas in this
dissertation.

Returning to the |x|/|y| example, here is how it plays out:
\begin{enumerate}
\item The variable |x| is inferred to have type |forall {a}. Int -> a -> a|.
\item When synthesizing the type of the expression |x| (in the body of |y|),
the inferred variable |a| is instantiated with |alpha|. The type of the
expression |x| is thus |Int -> alpha -> alpha|.
\item Right before the subsumption check, |x|'s type is generalized, thus
becoming |forall {a}. Int -> a -> a| again.
\item The subsumption relation checks whether |forall {a}. Int -> a -> a|
subsumes |Int -> forall b. b -> b|. This is indeed true, and the definition
for |y| is accepted.\footnote{Although not visible in \rul{Sub\_DeepSkol}
in \pref{fig:subsumption},
it is critical that $[[k2]]$ is skolemized \emph{before} $[[k1]]$ is
instantiated, lest we end up with the same scoping problem. This can be
seen in the full rule (\pref{app:subsumption}) with the fact that
we include $[[O1]]$ in the final generalization step. In contrast to
other potential pitfalls mentioned earlier, leaving $[[O1]]$ out of this
line does not imperil the soundness of elaboration; it is only a matter
of expressiveness of the source Haskell.}
\end{enumerate}

We have thus accepted our problem example and remain in line with the
declarative system proposed in my prior
work~\cite[Section 6.2]{visible-type-application}.

\section{Generalization}

There is one final aspect of the inference algorithm that requires study
before we look at the individual pieces: the generalization operation.\footnote{What I call generalization here is precisely what \citet[Section 7.5]{gundry-thesis} calls
``parameterisation'' and writes with $\nearrow$.} That said, in terms of understanding the \bake/ algorithm, having a strong grasp
on generalization is not terribly important; this is merely a technical
step needed to make the mathematics hold together.

Suppose we are synthesizing the type of a $\lambda$-expression |\x -> tau|.
We choose a unification variable $[[au]]$ for the type of |x|. We then must
put $[[&&xx :Rel au_{}]]$ into the context when synthesizing the type for |tau|.
Synthesizing this type will produce a unification telescope $[[O]]$. Now
we have a problem: what unification telescope will we return from synthesizing
the type of the entire $\lambda$-expression? It looks something like
$[[au :Irrel forall{}.Type{}, &&xx :Rel au_{}, O]]$ but, critically, that is not
a unification telescope, as that context contains a binding for an ordinary
\pico/ variable, |x|.

It might be tempting at this point simply to return a mixed telescope of
unification variables and \pico/ variables, and just to carry on. The problem
here is that we will lose track of the local scope of |x|. Perhaps something
later, outside of the $\lambda$-expression, will end up unifying with |x|---which
would be a disaster. No, we must get rid of it.

\begin{figure}
\ottdefnIIGen{}
\caption{\bake/'s generalization operation}
\label{fig:generalization}
\end{figure}

The solution is to generalize $[[O]]$ over |x|. This operation is written
$[[O +> &&xx :Rel Type{} ~> O';X]]$. (The mnemonic behind the choice of
$[[+>]]$ is that we are essentially moving the $[[&&xx :Rel Type{}]]$
binding to the right, past $[[O]]$.) The output unification telescope
$[[O']]$ binds the same unification variables as $[[O]]$, but each one
will be generalized with respect to |x|. The definition of this judgment
appears in \pref{fig:generalization}. The rules are a bit complicated
by the fact that we may generalize a unification variable binding multiple
times; both recursive rules thus assume a telescope $[[D']]$ that has
already been generalized.

The new construct $[[X]]$ is a \emph{generalizer}. It is a substitution-like
construct that maps unification variables to vectors, which you may recall
are lists of arguments $[[ps]]$. In this case, we simply use the domain
of $[[D]]$ as the vector, where my use of $[[dom(D)]]$ as a list of arguments
means to insert the irrelevance braces around irrelevantly-bound variables.
Generalizers are necessary because generalizing changes the type of
unification variables; we must then change the occurrences of them as well.

Generalizers operate like this:
\begin{definition*}[Generalizing {[\pref{defn:generalizer}]}]
A generalizer is applied postfix as a function. It operates homomorphically
on all recursive forms and as the identity operation on leaves other
than unification variables. Generalizing unification variables
is defined by these equations:
\[
\begin{array}{r@@{\quad}c@@{\quad}r@@{\;}l}
[[au |-> ps1 \in X]] & \Rightarrow & [[au_ps2[X] &= au_{{ps1,ps2}}]]\\
\text{otherwise} && [[au_ps[X] &= au_{{ps[X]}}]] \\
[[cu |-> ps1 \in X]] & \Rightarrow & [[cu_ps2[X] &= cu_{{ps1,ps2}}]]\\
\text{otherwise} && [[cu_ps[X] &= cu_{{ps[X]}}]]
\end{array}
\]
\end{definition*}
Just like the generalization judgment (\pref{fig:generalization}), the
generalization operation $[[ [X] ]]$ prepends the newly-generalized variables
to those already there.

\section{Type inference algorithm}

\begin{figure}[t!]
\[%\def\arraystretch{1.3}
\begin{array}{cl}
[[S;P |->ty _t ~> t : k -| O]] & \text{synthesize a type (no invis.~binders)} \\
[[S;P *|->ty _t ~> t : k -| O]] & \text{synthesize a type} \\
[[S;P |->ty _t : k ~> t -| O]] & \text{check a type (no invis.~binders)} \\
[[S;P *|->ty _t : k ~> t -| O]] & \text{check a type} \\
[[S;P |->pt _s ~> t -| O]] & \text{check a polytype (always with kind |Type|)} \\
[[S;P;rel *|->arg _t : k ~> p ; t -| O]] & \text{check an argument at relevance $[[rel]]$} \\
[[S;P;k0;t0 |->alt _alt : k ~> alt -| O]] & \text{check a case alt.~against an unknown type} \\
[[S;P;k0;t0 |->altc _alt : k ~> alt -| O]] & \text{check a case alt.~against a known type} \\
[[S;P |->q _qvar ~> a : k; vis -| O]] & \text{synth.~type of a bound var.} \\
[[S;P |->aq _aqvar ~> a : k -| O]] & \text{synth.~type of a bound var.~(w/o vis.~marker)} \\
[[S;P |->aq _aqvar : k ~> a : k'; x.t -| O]] & \text{check type of a bound var.~(w/o vis.~marker)} \\
[[|->pi quant ~> PI; rel]] & \text{interpret a quantifier} \\
[[|->fun k; rel1 ~> g; PI; a; rel2; k1; k2 -| O]] & \text{extract components of a function type} \\
[[S;P |->scrut _alts; k ~> g; D; H; ts -| O]] & \text{extract components of a scrutinee type} \\
[[|->inst_vis k ~> ps; k' -| O]] & \text{instantiate a type} \\
[[S;G |->decl _decl ~> x : k := t]] & \text{check a declaration} \\
[[S;G |->prog _prog ~> G'; theta]] & \text{check a program}
\end{array}
\]
\caption{\bake/ judgments}
\label{fig:bake-judgments}
\end{figure}

The schema of the judgments that define \bake/ appear in \pref{fig:bake-judgments}. I will not walk through each rule of each judgment to explain its inner
workings. As discussed in the introduction to this chapter, the individual
rules are largely predictable. They can be reviewed in their entirety in
\pref{app:inference-rules}. Instead, this section will call out individual
rules with interesting characteristics.

\subsection{No coercion abstractions}
\label{sec:no-coercion-abstractions}
\rae{TODO: Write me.}

\section{Metatheory}
\label{sec:bake-metatheory}

This chapter has explained the \bake/ algorithm in detail, but what
theoretical properties does it have? A type inference algorithm is often
checked for soundness and completeness against a specification. However, as
argued by \citet[Section 6.3]{outsidein}, lining up an algorithm such as
\bake/ against a declarative specification is a challenge. Instead of writing
a separate, non-algorithmic form of \bake/, I present three results in this
section:
\begin{itemize}
\item I prove that the elaborated \pico/ program produced by \bake/ is
indeed a well-typed \pico/ program. This result---which I call soundness---marks
an upper limit on the set of programs that \bake/ accepts. If it cannot
be typed in \pico/, \bake/ must reject.\footnote{I do not prove a correspondence
between the Haskell program and the \pico/ program produced by elaboration.
It would thus theoretically be possible to design \bake/ to accept
all input texts and produce a trivial elaborated program. But that wouldn't
be nearly as much fun, and I have not done so.}
\item In two separate subsections,
I argue that \bake/ is a conservative extension both
of the \outsidein/
algorithm and the SB algorithm of \citet{visible-type-application}.
That is, if \outsidein/ or SB accepts a program, so does \bake/.
This results suggests that a version of GHC
based on \bake/ will accept all Haskell programs currently accepted.
These arguments---I darenot quite call them proofs---are stated in less
formal terms than other proofs in this dissertation. While it is likely
possible
to work out the details fully, the presentation of the other systems and
of {\bake/}/{\pico/} differ enough that the translation between the systems
would be fiddly, and artifacts of the translation would obscure the main
point. The individual differences are discussed below.

These conservativity results provide a lower bound on the power of \bake/,
delcaring that some set of Haskell programs must be accepted by the algorithm.
\end{itemize}

The results listed above bound the power of the algorithm both from below
and from above, serving roughly as soundness and completeness results.
It is left as future work to define a precise specification of \bake/ and
prove that it meets the specification.

\subsection{Soundness}
\rae{Write me.}

\subsection{Conservativity with respect to \outsidein/}

I do not endeavor to give a full accounting of the \outsidein/ algorithm
here, instead referring readers to the original~\cite{outsidein}. I will
briefly explain judgments, etc., as they appear and refer readers to Figure
numbers from the original text.

\begin{table}
\begin{center}
\begin{tabular}{@@{}lccl@@{}}
\multicolumn{2}{c}{\outsidein/ construct} & \pico/ form & Notes \\ \hline
Axiom scheme & $[[QQ]]$ & $[[G]]$ & 
\begin{minipage}[t]{.45\textwidth}
\setlength{\baselineskip}{.8\baselineskip}
instances, etc.; implications are functions;
type family instances are via unfoldings \\[-1ex]
\end{minipage} \\
Given constraint & $[[Qg]]$, $[[Qr]]$ & $[[D]]$ & constraints are named in \pico/ \\
Wanted constraint & $[[Qw]]$ & $[[O]]$ & we must separate wanteds \& givens
\end{tabular}
\end{center}
\caption{Translation from \outsidein/ to \pico/}
\label{tab:oi-encode}
\end{table}

There are several mismatches between concepts in \outsidein/ and in
\pico/. Chief among these is that
\outsidein/ does not track unification variables in any
detail. All unification variables (and type variables, in general) in
\outsidein/ have kind $[[Type]]$, and thus there is no need for dependency
tracking. In effect, many judgments in \outsidein/ are parameterized by
an unwritten set of in-scope unification variables. We have no such luxury
of concision available in \bake/, and so there must be consideration
given to tracking the unification variables.

To partly bridge the gap between \outsidein/ and \bake/,
I define $[[encode]]$ which does the translation,
according to \pref{tab:oi-encode}. $[[encode]]$ing a construct from the left
column results in a member of the syntactic class
depicted in the middle column.

\outsidein/ differentiates
between algorithm-generated constraints $C$ and user-written ones
$Q$; the former contain implication constraints. I do not discern between
these classes, considering implication constraints simply as functions.
I will use $Q$ metavariables in place of \outsidein/'s $C$.\footnote{This
conflation of $Q$ and $C$ does not mean that Dependent Haskell is now
required to implement implication constraints; it would be easy to add
a post-typechecking pass (a ``validity'' check, in the vocabulary of the
GHC implementation) that ensures that no constraints have implications.}

A further difference between \outsidein/ and \bake/ is that the latter
is bidirectional. When \outsidein/ knows the type which it wishes to
assign to a term, it synthesizes the term's type and then emits an
equality constraint. In the comparison between the systems, we will
pretend that \bake/'s checking judgments do the same.

The fact that I must change my judgments does not imperil the practical
impact of the conservativity result---namely, programs that GHC accepts
today will still be accepted tomorrow. GHC already uses bidirectional
typechecking and so has already obviated the unidirectional aspect of
\outsidein/. However, in order to make a formal comparisons between that
published algorithm, it is helpful to restrict ourselves to a
unidirectional viewpoint.

A final difference is that \bake/ does elaboration, while \outsidein/
does not. I shall use the symbol $[[\cdot]]$ to denote an elaborated type
that is inconsequential in this comparison.

\subsubsection{Expressions}

\begin{claim*}[Expressions]
If $[[G oi|-> _t : k ~> Qw]]$ under axiom set $[[QQ]]$ and signature
$[[S]]$, then
$[[S;G,encode(QQ) |->ty _t ~> \cdot : k -| aus :Irrel forall{}.Type{}, encode(Qw)]]$
where $[[aus = fuv(k) \union fuv(Qw)]]$.
\end{claim*}

This claim relates \outsidein/'s $[[G oi|-> _t : k ~> Qw]]$ judgment
(Figures 6 and 13 from \citet{outsidein}) to \bake/'s synthesis $[[|->ty]]$
judgment. Note that the output $[[O]]$ from \bake/'s judgment must include
both the wanteds ($[[encode(Qw)]]$) and also any unification variables
required during synthesis ($[[aus]]$).

To argue this claim, we examine the different rules that make up
\outsidein/'s judgment, using structural induction.

\begin{description}
\item[Case \rul{VarCon}:]
\outsidein/ fully instantiates all variables, producing unification
variables for any quantified type variables and emitting wanted constraints
for any constraint in the variable's type. \bake/ does the same, via
its $[[|->inst]]$ judgment.

\item[Case \rul{App}:]
In this case, $[[_t = _t1 _t2]]$.
\outsidein/'s
is a fairly typicaly application form rule, but using constraints to assert
that the type of $[[_t1]]$ is indeed a function. \Bake/ works similarly,
using its $[[|->fun]]$ judgment to assert that a type is a $\Pi$-type.
Of course, \outsidein/'s treatment of $[[_t2]]$ uses synthesis while
\bake/'s uses its checking judgment.

\item[Case \rul{Abs}:]
This rule coresponds quite closely to \bake/'s \rul{ITy\_Lam} rule.
Note that \outsidein/ does not permit annotations on $\lambda$-bound
variables, simplifying the treatment of abstractions. Furthermore,
\bake/ must use its generalization judgment (written with $[[+>]]$)
to handle its unification variables, while \outsidein/ does not need
to have this complication.

\item[Case \rul{Case}:]
Contrast \outsidein/'s rule with \bake/'s:
\[
\ottdrule{%
\ottpremise{\Gamma \varrow e : \tau \rightsquigarrow C \quad \beta, \overline{\gamma} \text{ fresh}}%
\ottpremise{K_i{:}\forall \overline{a} \overline{b}_i . Q_i \Rightarrow \overline{\upsilon}_i \to \texttt{T}\, \overline{a} \quad \overline{b}_i \text{ fresh}}%
\ottpremise{\Gamma,(\overline{x_i{:}[\overline{a \mapsto \gamma}]\upsilon_i}) \varrow e_i : \tau_i \rightsquigarrow C_i \quad \overline{\delta}_i = \mathit{fuv}(\tau_i,C_i) - \mathit{fuv}(\Gamma,\overline{\gamma})}%
\ottpremise{C'_i = \left\{ \begin{array}{ll}
C_i \wedge \tau_i \sim \beta & \text{if $\overline{b}_i = \epsilon$ and $Q_i = \epsilon$} \\
\exists \overline{\delta}_i.([\overline{a \mapsto \gamma}]Q_i \supset C_i \wedge \tau_i \sim \beta) & \text{otherwise} \end{array} \right.}}%
{\Gamma \varrow \texttt{case}\,e\,\texttt{of}\,\{\overline{K_i\,\overline{x}_i \to e_i}\} : \beta \rightsquigarrow C \wedge (\texttt{T}\,\overline{\gamma} \sim \tau) \wedge (\bigwedge C'_i)}%
{\rul{Case}}
\]
%% \ottdrule{%
%% \ottpremise{
%% \Gamma \varrow e : \tau \rightsquigarrow C \quad \beta, \overline{\gamma} \text{ fresh} \quad C' = (\texttt{T}\,\overline{\gamma} \sim \tau) \wedge C}%
%% \ottpremise{\text{for each } K_i\,\overline{x}_i \to e_i \text{ do}}%
%% \ottpremise{\quad K_i{:}\forall \overline{a}.\overline{\upsilon}_i \to \texttt{T}\, \overline{a} \in \Gamma \quad \Gamma, (\overline{x_i{:}[\overline{a \mapsto \gamma}]\upsilon_i}) \varrow e_i : \tau_i \rightsquigarrow C_i}%
%% \ottpremise{\quad C'_i = C_i \wedge \tau_i \sim \beta}}%
%% {\Gamma \varrow \texttt{case}\,e\,\texttt{of}\,\{\overline{K_i \overline{x}_i \to e_i}\} : \beta \rightsquigarrow C' \wedge (\bigwedge C'_i)}%
%% {\rul{Case}}
\[
\ottdruleITyXXCase{}
\]
\[
\ottdruleIAltXXCon{}
\]
The first premises line up well, with both checking the scrutinee. Both
rules then must ensure that the scrutinee's type is headed by a type
constant (\texttt{T} in \outsidein/, $[[H]]$ in \bake/). This is done
via the emission of a constraint in \outsidein/ (the $\texttt{T}\,\overline{\gamma} \sim \tau$ constraint in the conclusion)
and the use of $[[|->scrut]]$ in \bake/. One reason for a difference in
treatment here is that \bake/ wishes to use any information available because
of the existence of its checking judgment, whereas \outsidein/ is free to
invent new, uninformative unification variables ($\overline{\gamma}$).
This difference makes \bake/ produce the unification variables only when
the scrutinee's type ($[[k0]]$) is not already manifestly the right shape.

Both rules then check the individual alternatives, which have to look
up the constructor ($K_i$ and $[[H]]$, respectively) in the environment.
\Pico/ gathers the three sorts of existentials together in $[[D2]]$;
\outsidein/ expands this out to the $\overline{b}_i$, $Q_i$, and $\overline{\upsilon}_i$. \outsidein/ does not permit unsaturated matching, so we can
treat $[[D4]]$ as empty. \outsidein/ does not consider scoped type variables;
it thus only brings in $\overline{x}_i$ of types $\overline{\upsilon}_i$
while checking each alternative; \bake/ brings all of $[[D3]]$ into scope.
\outsidein/ checks the synthesized type of each alternative, $\tau_i$ against
the overall result type $\beta$; \bake/ ensures the types of the alternatives
line up by using a checking judgment against the result kind $[[k]]$. (Note
that, like \outsidein/, this result kind is a unification variable. We can
see that the $[[k]]$ in \rul{IAlt\_Con} is the $[[au]]$ from \rul{ITy\_Case}.)

The constraint $C'_i$ emitted by \outsidein/ is delicately constructed.
If there are no existential type variables and no local constraints,
\outsidein/ emits a simple constraint. Otherwise, it has to emit
an implication constraint. \outsidein/'s implication constraints
allow unification variables to be local to a certain constraint; the
treatment of implication constraints is somewhat different than that of
simple constraints. In particular, when solving an implication constraint,
any unification variables that arose outside of that implication are
considered untouchable---they cannot then be unified.
(See \pref{sec:untouchability}.) In order
to avoid imposing the untouchability restriction, \outsidein/ makes a
simple constraint when possible. Due to \bake/'s more uniform treatment
of implications, this distinction is not necessary; untouchability is
informed by the order of unification variables in the context passed
to the solver.

\Bake/ makes its version of implication constraints via the generalization
judgment (written with $[[+>]]$). All of the constraints generated while
checking the alternative are quantified over variables in $[[D3]]$; this
precisely corresponds to the apapearance of the $Q_i$ to the left of the
$\supset$ in \outsidein/'s constraint $C'_i$. (Recall that $Q_i$ is a
component of \bake/'s $[[D3]]$.) \Bake/ also adds another equality
assumption into its $[[D3']]$; this equality has to do with dependent
pattern matching (\pref{sec:dependent-pat-match}) and has no place in
the non-dependent language of \outsidein/.

\item[Case \rul{Let}:]
Other than \bake/'s generalization step, these rules line up perfectly.

\item[Cases \rul{LetA} and \rul{GLetA}:]
These cases cover annotated \ottkw{let}s, where the bound variable is
also given a type. I do not consider this form separately, instead
preferring to use the checking judgments to handle this case.
\end{description}

\subsubsection{The solver}

\begin{property*}[Solver]
If $[[QQ; Qg; aus1 oi|->solv Qw ~> Qr; Z]]$ where $[[S]]$ and $[[G]]$ capture
the signature and typing context for the elements of that judgment,
then $[[S;G, encode(QQ), encode(Qg) |->solv aus1 :Irrel forall{}.Type{}, encode(Qw) ~> as2:Irrel Type{}, encode(Qr)[as2/aus2]; as2/aus2, Z]]$,
where the $[[as2]]$ are fresh replacements for the $[[aus2]]$ which are free
in $[[Qr]]$ or unconstrained variables in $[[aus1]]$.
\end{property*}

This property is a bit more involved than we would hope, but all of the complication
deals with \bake/'s requirement of tracking unification variables more carefully
than does \outsidein/.
Underneath all of the faffing about with unification variables, the key
point here is that \bake/'s solver will produce the same residual constraint
$[[Qr]]$ as \outsidein/'s and the same zonking substitution $[[Z]]$.

I do not try to argue this property directly, as I do not present the
implementation for the solver. However, this property shows a natural
generalization of the solver in an environment that includes dependencies
among variables. Indeed, GHC's implementation of the solver already handles
such dependency.

\subsubsection{Programs}

\begin{claim*}[\rul{Bind}]
\label{lem:oi-bind}
If $[[G oi|-> _t : k ~> Qw]]$ and
$[[QQ; empt; fuv(k) \union fuv(Qw) oi|->solv Qw ~> Qr; Z]]$,
then
$[[S;G, encode(QQ) |->decl x := _t ~> x : UPI_Inf as :Irrel Type{}. (UPI_Inf encode(Qr). k[Z])[as/aus] := t]]$ for some $[[t]]$, where $[[aus = fuv(k[Z]) \union fuv(Qr)]]$ and $[[as]]$ are fresh replacements for the $[[aus]]$.
\end{claim*}

This claim relates \outsidein/'s \rul{Bind} rule (Figure 12) to \bake/'s
\rul{IDecl\_Synthesize} rule. It is a consequence of the claim on expressions
and the property above of the solver.

%% \begin{proof}
%% We will use \rul{IDecl\_Synthesize}.
%% \pref{lem:oi-expr} tells us
%% $[[S;G, encode(QQ) |->ty _t ~> t : k -| aus1:Irrel forall{}.Type{}, encode(Qw)]]$
%% where $[[aus1 = fuv(k) \union fuv(Qw)]]$.
%% Then \pref{prop:oi-solv} tells us
%% $[[S;G, encode(QQ) |->solv aus1 :Irrel forall{}.Type{}, encode(Qw) ~> as2:Irrel Type{}, encode(Qr)[as2/aus2]; as2/aus2, Z]]$, where the $[[as2]]$ are fresh replacements for the $[[aus2]]$ which are the free
%% in $[[Qr]]$ or unconstrained variables in $[[aus1]]$. The latter set is
%% precisely $[[fuv(k[Z])]]$.
%% \rul{IDecl\_Synthesize} then tells us
%% $[[S;G, encode(QQ) |->decl x := _t ~> x : UPI_Inf as2:Irrel Type{}, encode(Qr)[as2/aus2]. (k[Z]) := t']]$
%% for some $[[t']]$ (whose details are irrelevant).
%% We are done.
%% \end{proof}

\begin{claim*}[Conservativity over \outsidein/]
If $[[QQ; G oi|-> _prog]]$, $[[_prog]]$ contains no annotated bindings,
and $[[S]]$ captures the signature of the
environment $[[_prog]]$ is checked in,
then $[[S; G, encode(QQ) |->prog _prog ~> G'; theta]]$.
\end{claim*}

This claim relates the overall action of the \outsidein/ algorithm
(Figure 12) to \bake/'s algorithm for checking programs. It follows
directly from the previous claim.

Because of this, I believe that any program without top-level annotations
accepted by \outsidein/ is also accepted by \bake/.

\subsection{Conservativity with respect to SB}

Here, I compare \bake/ with the bidirectional algorithm (called SB)
in Figure 8 of
\citet{visible-type-application}. That algorithm is proven to be
a conservative extension both of Hindley-Milner inference and also
of the bidirectional algorithm presented by \citet{practical-type-inference}.
This SB algorithm, along with \outsidein/,
 is part of the basis for the algorithm currently
implemented in GHC 8. 

\section{Discussion}
