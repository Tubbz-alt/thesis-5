%% -*- mode: LaTeX compile-command: "cd ..; make compile" -*-

%if style == newcode
%include rae.fmt

\begin{code}

import Data.Maybe ( isNothing )
import Data.Kind
import Data.Singletons hiding ( Proxy )
import Prelude

\end{code}

%endif

\chapter[Type inference and elaboration]{Type inference and elaboration, or How to \bake/ a \pico/}
\label{cha:type-inference}

\pref{cha:dep-haskell} presents the additions to modern Haskell to make
it Dependent Haskell, and \pref{cha:pico} presents \pico/, the internal
language to which we compile Dependent Haskell programs. This chapter
formally
relates the two languages by defining a type inference/elaboration algorithm,\footnote{I refer to \bake/ variously as an elaboration algorithm, a type inference
algorithm, and a type checking algorithm. This is appropriate, as it is all three.
In general, I do not differentiate between these descriptors.}
\bake/,
checking Dependent Haskell code and producing a well-typed \pico/ program.

At a high level, \bake/ is unsurprising. It simply combines the ideas
of several pieces of prior
work~\cite{outsidein,visible-type-application,gundry-thesis} and targets
\pico/ as its intermediate language. Despite its strong basis in prior work,
\bake/ exhibits a few novelties:
\begin{itemize}
\item Perhaps its biggest innovation is how
it decides between dependent and non-dependent pattern matching depending
on whether the algorithm is in checking or synthesis mode. (See also
\pref{sec:bidir-dependent-pattern-match}.)
%format mathrms = "[[_s]]"
\item It turns out that checking the annotated expression
|(\ (x :: mathrms) -> ...) :: forall x -> ...| depends on whether
or not the type annotation describes a dependent function. This came
as a surprise. See \pref{sec:annotated-lambdas}.
\item The subsumption relation allows an unmatchable function to be
subsumed by a matchable one. That is, a function expecting an unmatchable
function |a -> b| can also accept a matchable one |a !-> b|.
\end{itemize}

After presenting the elaboration algorithm, I discuss the metatheory
in \pref{sec:bake-metatheory}. This section include a soundness result
that the \pico/ program produced by \bake/ is well-typed. It also relates
\bake/ both to \outsidein/ and the bidirectional type system (``System SB'')
from \citet{visible-type-application}, arguing that \bake/ is a conservative
extension of both.

Full statements of all theorems and definitions, with proofs, appear
in \pref{app:inference}.

\section{Overview}
\label{sec:solv-spec}
\label{sec:dependent-compose}

\Bake/ is a bidirectional~\cite{local-type-inference}
constraint-generation algorithm~\cite{remy-attapl}. It walks over
the input syntax tree and generates constraints, which are later solved.
It can operate in either a synthesis mode (when the type is unknown)
or in checking mode (when the expected type of an expression is known). Like
prior work~\cite{outsidein,gundry-thesis}, I leave the details of the solver
unspecified; any solver that obeys the properties described in
\pref{sec:solver-properties} will do. In practice, the solver will be the one
currently implemented in GHC. Despite the fact that the dependency tracking
described here is omitted from \citet{outsidein}, the most detailed description of
GHC's solver,\footnote{In the text of the paper describing
  \outsidein/~\cite{outsidein}, the authors separate out the constraint
  generation from the solver. They call the constraint-generation algorithm
  \outsidein/ and the solver remains unnamed. I use the moniker \outsidein/ to
  refer both to the constraint-generation algorithm and the solver.}
the solver as implemented does indeed do dependency tracking and should
support all of the innovations described in this chapter.

Constraints in \bake/ are represented by \emph{unification telescopes}, which
are a list of possibly-dependent unification variables,\footnote{Depending on
  the source, various works in the literature refer to unification variables
  as existential variables (e.g., \cite{simple-bidirectional}) or
  metavariables (e.g., \cite{gundry-thesis} and the GHC source code). I prefer
  unification variables here, as I do not wish to introduce confusion with
  existentials of data constructors nor the metavariables of my developed
  metatheory.} with their types. Naturally, there are two sorts of unification
variables: types $[[au]]$ and coercions $[[cu]]$. The solver finds concrete
types to substitute in for unification variables $[[au]]$ and concrete
coercions to substitute in for unification variables $[[cu]]$. Implication
constraints, a key innovation of \citet{outsidein}, are handled by classifying
unification variables by quantified kinds and propositions. See
\pref{sec:quantified-kinds-and-props}.

The algorithm is stated as several judgments of the following general form:
\[
[[S]];[[P]] [[|->]] \mathit{inputs} [[~>]] \mathit{outputs} [[-|]] [[O]]
\]
Most judgments are parameterized by a fixed signature $[[S]]$ that defines
the datatypes that are in scope.\footnote{I do not consider in this
dissertation how these
signatures are formed. To my knowledge, there is no formal presentation
of the type-checking of datatype declarations, and I consider formalizing
this process and presenting an algorithm to be important future work.}
The context $[[P]]$ is a generalization of contexts $[[G]]$; a context
$[[P]]$ contains both \pico/ variables and unification variables.
Because this is an algorithmic treatment of type inference, the notation
is careful to separate inputs from outputs. Everything to the left of
$[[~>]]$ is an input; everything to the right is an output. Most judgments
also produce an output $[[O]]$, which is a unification telescope, containing
bindings for only unification variables. This takes the place of the emitted
constraints seen in other constraint-generation algorithms. It also serves
as a context in which to type-check future parts of the syntax.

The solver's interface looks like this:
\[
[[S;P |->solv O ~> D ; Z]]
\]
That is, it takes as inputs the current environment and a unification telescope.
It produces outputs of $[[D]]$, a telescope of variables to quantify over,
and $[[Z]]$, the \emph{zonker} (\pref{sec:zonking}),
which is an idempotent
substitution from unification variables to other types/coercions.
To understand the output $[[D]]$, consider checking the declaration
|y = \x -> x|. The variable |x| gets assigned a unification variable type
$[[au]]$. No constraints then get put on that type. When trying to solve
the unification telescope $[[au :Irrel forall{}.Type{}]]$, we have nothing to do.
The answer is, of course, to generalize. So we get $[[D = a :Irrel Type{}]]$
and $[[Z = a/au]]$. In the constraint-generation rules for declarations,
the body of a declaration and its type are generalized over $[[D]]$.
(See \rul{IDecl\_Synthesize} in \pref{sec:idecl}.)

Writing a type inference algorithm for a dependently typed language presents
a challenge in that the type of an expression can be very intricate. Yet
we still wish to infer (simple) types for unannotated expressions. To resolve
this tension, \bake/ adheres to the following:
\theoremstyle{plain}
\newtheorem*{guidingprinciple}{Guiding Principle}
\begin{guidingprinciple}
In absence of other information, infer a simple type.
\end{guidingprinciple}
\begin{guidingprinciple}
Never make a guess.
\end{guidingprinciple}
For example, consider inferring a type for
\begin{code}
compose f g = \x -> f (g x)
\end{code}
The function |compose| could naively be given either of the following types:
\begin{spec}
compose  ::  (b -> c) -> (a -> b) -> (a -> c)
compose  ::  forall  (a :: Type)
                     (b :: a -> Type) 
                     (c :: forall (x :: a) -> b x -> Type)
         .   pi      (f :: forall (x :: a). pi (y :: b x) -> c x y)
                     (g :: pi (x :: a) -> b x)
                     (x :: a)
         ->  c x (g x)
\end{spec}
%if style == newcode
\begin{code}
test4 = compose :: (b -> c) -> (a -> b) -> (a -> c)
{-
dcomp :: forall (a :: Type)
                (b :: a ~> Type)
                (c :: forall (x :: a). Proxy x ~> b @@ x ~> Type)
                (f :: forall (x :: a) (y :: b @@ x). Proxy x ~> Proxy y ~> c @@ (!Proxy :: Proxy x) @@ y)
                (g :: forall (x :: a). Proxy x ~> b @@ x)
                (x :: a).
         Sing f
      -> Sing g
      -> Sing x
      -> c @@ ('Proxy :: Proxy x) @@ (g @@ ('Proxy :: Proxy x))
dcomp f g x = f `applySing` _
-}
\end{code}
%endif
However, we surely want inference to produce the first one. If inference
did not tend toward simple types, there would be no hope of retaining
principal types in the system. I do not prove that \bake/ infers principal
types, as doing so is meaningless without some non-deterministic specification
of the type system, which is beyond the scope of this work. However, I still
wish to design Dependent Haskell with an eye toward establishing a
principal types result in the future. Just like how inferring only rank-1 types
still allows for higher-rank types in a bidirectional type system~\cite{practical-type-inference}, it is my hope that inferring only simple types will
allow for Dependent Haskell to retain principal types.

The second guiding principle is that \bake/ should never make guesses.
Guesses, after all, are sometimes wrong. By ``guess'' here, I mean that
the algorithm and solver should never set the value of a unification variable
unless doing so is the only possible way an expression can be well typed.
Up until this point, GHC's type
inference algorithm has resolutely refused to guess. This decision manifests
itself, among other places, in GHC's inability to work with a function |f :: F a -> F a|, where |F| is a type function.\footnote{Unless |F| is
known to be injective~\cite{injective-type-families}.} The problem is that,
from |f 3|, there is no way to figure out what |a| should be, and GHC will
not guess the answer.

A key consequence of not making any guesses is that \bake/ (more accurately,
the solver it calls) does no higher-order unification. Consider this example:
\begin{spec}
fun :: a -> (f $ a)   -- NB: |f| is not a matchable function

bad :: Bool -> Bool
bad x = fun x
\end{spec}
%if style == newcode
\begin{code}
fun = undefined
type a $ b = a b
\end{code}
%endif
In the body of |bad|, it is fairly clear that we should unify |f| with the
identity function. Yet the solver flatly refuses, because doing so amounts to a
guess, given that there are
many ways to write the identity function.\footnote{Note that my development
does not natively support functional extensionality, so that these different
ways of writing an identity function are not equal to one another.}

In my choice to avoid higher-order unification, my design diverges from the
designs of other dependently typed languages, where higher-order unification
is common. Time will tell whether the predictability gotten from avoiding
guesses is worth the potential annoyance of lacking higher-order unification.
Avoiding guesses is also critical for principal types. See
\citet[Section 3.6.2]{outsidein} for some discussion.

Now that we've seen the overview, let's get down to details.

\section{Haskell grammar}

\begin{figure}
\[
\begin{array}{rcl@@{\quad}l}
[[_t]],[[_k]] &\bnfeq& [[a]] \bnfor [[\ _qvar . _t]] \bnfor [[/\ _qvar. _t]] \bnfor [[_t1 _t2]] \bnfor [[_t1 @_t2]] \bnfor [[_t :: _s]] & \text{type/kind} \\
&\bnfor& [[case _t of _alts]] \bnfor [[_t1 -> _t2]] \bnfor [[_t1 '-> _t2]] \bnfor [[fix _t]]  \\
&\bnfor& [[let x := _t1 in _t2]] \\
[[_qvar]] &\bnfeq& [[_aqvar]] \bnfor [[@_aqvar]] & \text{quantified variable} \\
[[_aqvar]] &\bnfeq& [[a]] \bnfor [[a :: _s]] & \text{quantified variable (w/o vis.)} \\
[[_alt]] &\bnfeq& [[_pat -> _t]] & \text{case alternative} \\
[[_pat]] &\bnfeq& [[H xs]] \bnfor [[_]] & \text{pattern} \\
[[_s]] &\bnfeq& [[quant _qvar. _s]] \bnfor [[_t => _s]] \bnfor [[_t]] & \text{type scheme/polytype} \\
[[quant]] &\bnfeq& [[forall]] \bnfor [['forall]] \bnfor [[pie]] \bnfor [['pie]] & \text{quantifier} \\[1ex]
[[_decl]] &\bnfeq& [[x :: _s := _t]] \bnfor [[x := _t]] & \text{declaration} \\
[[_prog]] &\bnfeq& [[empty]] \bnfor [[_decl; _prog]] & \text{program}
\end{array}
\]
\caption{Formalized subset of Dependent Haskell}
\label{fig:formal-haskell-grammar}
\end{figure}

I must formalize a slice of Dependent Haskell in order to describe an
elaboration procedure over it. The subset of Haskell I will consider is
presented in \pref{fig:formal-haskell-grammar}. Note that all Haskell
constructs are typeset in upright Latin letters; this is to distinguish
these from \pico/ constructs, typeset in italics and often using Greek
letters.

The version of Dependent Haskell presented here differs in a few details
from the language presented in \pref{cha:dep-haskell}. These differences
are to enable an easier specification of the elaboration algorithm. Translating
between the ``real'' Dependent Haskell of \pref{cha:dep-haskell} and this
version can be done by a preprocessing step. Critically,
(but with one exception)
no part of
this preprocessor needs type information. For example,
|forall a b. ...| is translated to $[[forall @&&ax. forall @&&bx. ...]]$
so that it is easier to consider individual bound variables.

The exception to the irrelevance of type information is in dealing with
pattern matches. Haskell pattern matches can be nested, support guards,
perhaps view patterns,
perhaps pattern synonyms~\cite{pattern-synonyms}, etc. However, translating
such a rich pattern syntax into a simple one
is a well-studied problem with widely-used
solutions~\cite{augustsson-compiling-pattern-matching,wadler-pattern-matching}
and I thus consider the algorithm as part of the preprocessor and do 
not consider this further.

\subsection{Dependent Haskell modalities}

Let's now review some of the more unusual annotations in Dependent Haskell,
originally presented in \pref{cha:dep-haskell}. Each labeled paragraph
 below describes
an orthogonal feature (visibility, matchability, relevance).

\paragraph{The $\at$ prefix}
Dependent Haskell uses an $\at$ prefix to denote an argument that would
normally be invisible. It is used in two places in the grammar:
\begin{itemize}
\item An $\at$-sign before an argument indicates that the argument may
be omitted and left to inference. This follows the treatment in my prior
work on invisible arguments~\cite{visible-type-application}.
\item An $\at$-sign before a quantified variable (in the definition for
$[[_qvar]]$) indicates that the actual argument may be omitted when
calling a function. In a $\lambda$-expression, this would indicate
a pattern that matches against an invisible argument
(\pref{sec:visible-type-pat}). In a $\Pi$- or $\forall$-expression, the
$\at$-sign is produced by the preprocessor when it encounters a
|forall ... .| or |pi ... .| quantification.
\end{itemize}

\paragraph{Ticked quantifiers}
Three of the quantifiers that can be written in Dependent Haskell
come in two varieties: ticked and unticked. An ticked quantifier
serves in the type of matchable (that is, generative and injective)
functions, whereas the unticked quantifier describes an unrestricted
function space. Recall that type constructors and data constructors
are typed by matchable functions, whereas ordinary $\lambda$-expressions
are not.

\paragraph{Relevance}
The difference between |forall| and |pi| in Dependent Haskell is that
the former defines an irrelevant abstraction (fully erased during compilation)
while the latter describes a relevant abstraction (retained at runtime).
In terms, an expression introduced by $\lambda$ is a relevant abstraction;
one introduced by $\Lambda$ is an irrelevant one.

\subsection{|let| should not be generalized}

Though the formalized Haskell grammar includes |let|, I will take the
advice of \citet{let-should-not-be-generalised} that |let| should not
be generalized. As discussed at some length in the work cited, local,
generalized |let|s are somewhat rare and can easily be generalized by
a type signature. For all the same reasons articulated in that work,
generalizing |let| poses a problem for \bake/. We thus live with an
ungeneralized |let| construct.

\subsection{Omissions from the Haskell grammar}

There are two notable omissions from the grammar in \pref{fig:formal-haskell-grammar}.

\paragraph{Type constants} 
The Haskell grammar contains no
production for $[[H]]$, a type constant. This is chiefly because
type constants must be saturated with respect to universals in \pico/,
whereas we do not need this restriction in Haskell. Accordingly, type
constants are considered variables that expand to type constants
that have been $\eta$-expanded to take their universal arguments in
a curried fashion.

\paragraph{Recursive |let|}
Following the decision not to include a \keyword{letrec} construct in
\pico/ (\pref{sec:no-letrec}),
the construct is omitted from the formalized subset of Haskell as well.
Having a formal treatment of \keyword{letrec} would require a formalization
of Haskell's consideration of polymorphic recursion~\cite{meertens-polymorphic-recursion,mycroft-polymorphic-recursion,henglein-polymorphic-recursion}, whereby
definitions with type signatures can participate in polymorphic recursion while
other definitions cannot. In turn, this would require a construct where
a polymorphic function is treated monomorphically in a certain scope and
polymorphically beyond that scope.\footnote{Readers familiar with the
internals of GHC may recognize its |AbsBinds| data constructor in this
description. Formalizing all of its intricacies would indeed be required
to infer the type of a \keyword{letrec}.} The problems faced here are not
unique to (nor made particularly worse by) dependent types. I thus have
chosen to exclude this construct for simplicity.

\paragraph{}
We have now reviewed the source language of \bake/, and the previous
chapter described its target language, \pico/. I'll now fill in the gap
by introducing the additions to the grammar needed to describe the inference
algorithm.

\section{Unification variables}
\label{sec:quantified-kinds-and-props}

\begin{figure}
Metavariables:
\[
\begin{array}{rl@@{\qquad}rl}
[[au,bu]] & \text{unification type variable} & [[cu]] & \text{unification coercion variable}
\end{array}
\]
Grammar extensions:
\[
\begin{array}{rcl@@{\quad}l}
[[t]] &\bnfeq& \ldots \bnfor [[au _ ps]] & \text{type/kind} \\
[[g]] &\bnfeq& \ldots \bnfor [[cu _ ps]] & \text{coercion} \\[1ex]
[[zu]] &\bnfeq& [[au]] \bnfor [[cu]] & \text{unification variable} \\
[[Z]] &\bnfeq& [[empty]] \bnfor [[Z, forall zs. t / au]] \bnfor [[Z, forall zs. g/cu]] & \text{zonker (\pref{sec:zonker})} \\
[[X]] &\bnfeq& [[empty]] \bnfor [[X, zu |-> ps]] & \text{generalizer (\pref{sec:generalizer})} \\[1ex]
%
[[u]] &\bnfeq& [[au :rel forall D. k]] \bnfor [[cu : forall D. phi]] & \text{unif.~var.~binding} \\
[[O]] &\bnfeq& [[empty]] \bnfor [[O, u]] & \text{unification telescope} \\
[[P]] &\bnfeq& [[empty]] \bnfor [[P, d]] \bnfor [[P, u]] & \text{typing context} \\%[1ex]
%[[J]] &\bnfeq& \multicolumn{2}{l}{\text{stand-in for an arbitrary judgment (\pref{sec:J})}}
\end{array}
\]
I elide the $[[forall]]$ when the list of variables or telescope quantified
over would be empty.
\caption{Additions to the grammar to support \bake/.}
\label{fig:bake-grammar}
\end{figure}

The extensions to the grammar to support inference are in \pref{fig:bake-grammar}.
These extensions all revolve around supporting unification variables, which
are rather involved. One might think that unification variables need not be
so different from ordinary variables; constraint generation could produce a
telescope of these unification variables and solving simply produces a
substitution. However, this naive view does not work out
because of unification variable generalization.\footnote{The treatment
of unification variables throughout \bake/ is essentially identical to
the treatment by \citet{gundry-thesis}, which is itself closely based
on the work of \citet{simple-bidirectional}.}

Consider a $\lambda$-abstraction over the variable $[[x]]$. When doing
constraint generation inside of the $\lambda$, the kinds of fresh unification
variables might mention $[[x]]$. Here is a case in point, which will serve
as a running example:
%
\begin{spec}
poly :: forall j (b :: j) -> ...

example = \ k a -> poly k a
\end{spec}
%
Type inference can easily discover that the kind of |a| is |k|. But in order
for the inference algorithm to do this, it must be aware that |k| is in
scope before |a| is. Note that when we call the solver after type-checking
the entire body of |example|, |k| is \emph{not} in scope. Thus, as we produce
the unification telescope during constraint-generation over the body of
|example|, we must somehow note that the unification variable $[[au]]$
(the type of |a|) can mention |k|.

This means that unification variable bindings are quantified over a
telescope $[[D]]$. (You can see this in the definition for $[[u]]$
in \pref{fig:bake-grammar}.) In the vocabulary of \outsidein/, the
bindings in $[[D]]$ are the \emph{givens} under which a unification
variable should be solved for.

\subsection{Zonking}
\label{sec:zonking}
\label{sec:zonker}

Solving produces a substitution from unification variables to types/coercions.
Following the nomenclature within GHC, I call applying this substitution
\emph{zonking}. The substitution itself, written $[[Z]]$, is called a
\emph{zonker}.

Zonkers pose a naming problem. Consider solving
to produce the zonker for |example|, above. Suppose the type of |a| is
assigned to be $[[au]]$. We would like to zonk $[[au]]$ to |k|.
However, as before, |k| is out of scope when solving for $[[au]]$. We thus
cannot just write $|k|/[[au]]$, as that would violate the Barendregt
convention, where we can never name a variable that is out of scope
(as it might arbitrarily change due to $\alpha$-renaming).

The solution to this problem is to have all occurrences of unification
variables applied to vectors $[[ps]]$.\footnote{Recall that $[[p]]$ is
a metavariable that can stand for either a type or a coercion. Thus
$[[ps]]$ is a mixed list of types and coercions, suitable for substituting
in for a list of type/coercion variables $[[zs]]$.} When we zonk a
unification variable occurrence $[[au _ ps]]$, the vector $[[ps]]$ is
substituted for the variables in the telescope $[[D]]$ that $[[au]]$'s kind
is quantified over.

Here is the formal definition of zonking:

\begin{definition*}[Zonking {[\pref{defn:zonking}]}]
A zonker can be used as a postfix function. It operates homomorphically
on all recursive forms and as the identity operation on leaves other
than unification variables. Zonking unification variables
is defined by these equations:
\[
\begin{array}{r@@{\quad}c@@{\quad}r@@{\;}l}
[[forall zs. t/au \in Z]] & \implies & [[au_ps[Z] &= t[ps[Z]/zs ] ]] \\
\text{otherwise} && [[au_ps[Z] &= au_{{ps[Z]}} ]] \\[1ex]
[[forall zs. g/cu \in Z]] & \implies & [[cu_ps[Z] &= g[ps[Z]/zs ] ]] \\
\text{otherwise} && [[cu_ps[Z] &= cu_{{ps[Z]}} ]]
\end{array}
\]
\end{definition*}

In our example, we would say that |a| has the type $[[au_&&kx]]$, where
$[[au :Irrel forall &&kx :Irrel Type{}. Type{}]]$. The solver will create
a zonker with the mapping $[[forall &&jx. &&jx/au]]$ (where I have
changed the variable name for demonstration). This will zonk
$[[au_&&kx]]$ to become $[[&&jx[&&kx/&&jx] ]]$ which is, of course
$[[&&kx]]$ as desired.

Note that the quantification we see here is very different from normal
$\Pi$-quantification in \pico/. These quantifications are fully second-class
and may be viewed almost as suspended substitutions.

\subsection{Additions to \pico/ judgments}

\begin{figure}
\ottdefnUTy{}
\ottdefnUCo{}
\ottdefnUCtx{}
\caption{Extra rules in \pico/ judgments to support unification variables}
\label{fig:bake-pico-judgments}
\end{figure}

The validity and typing judgments in \pico/ all work over signatures
$[[S]]$ and contexts $[[G]]$. In \bake/, however, we need to be able
to express these judgments in an environment where unification variables
are in scope. I thus introduce mixed contexts $[[P]]$, containing both
\pico/ variables and unification variables.

Accordingly, I must redefine all of the \pico/ judgments to support
unification variables in the context. These judgments are written with
a $[[|=]]$ turnstile in place of \pico/'s $[[|-]]$ turnstile. There are
also several new rules that must be added to support unification variables.
These rules appear in \pref{fig:bake-pico-judgments}.

Note the rules \rul{Ty\_UVar} and \rul{Co\_UVar} that support unification
variable occurrences. The unification variables are applied to vectors
$[[ps]]$ which must match the telescope $[[D]]$ in the classifier for
the unification variable. In addition, this vector is substituted directly
into the unification variable's kind.

These definitions support all of the properties proved about the original
\pico/ judgments, such as substitution and regularity. The statements
and proofs are in \pref{app:inference}.

\subsection{Untouchable unification variables}
\label{sec:untouchability}

\citet[Section 5.2]{outsidein} introduces the notion of \emph{touchable}
unification variables, as distinct from \emph{untouchable} variables. Their
observation is that it is harmful to assign a value to a ``global'' unification
variable when an equality constraint is in scope. ``Global'' here means that
the unification variable has a larger scope than the equality constraint.
We call the ``local'' unification 
variables touchable, and the ``global'' ones untouchable. \outsidein/ must
manually keep track of touchability; the set of touchable unification variables
is an extra input to its solving judgment.

In \bake/, on the other hand, tracking touchability is very easy with its
use of unification telescopes: all unification variables quantified by the
same equality constraints as the constraint under consideration are touchable;
the rest are untouchable.

To make this all concrete, let's look at a concrete example (taken from
\citet{outsidein}) where the notion of touchable variables is beneficial.

Suppose we have this definition:
%
\begin{code}
data T a where
  K :: (Bool ~ a) => Maybe Int -> T a
\end{code}
%
I have written this GADT with an explicit equality constraint in order to
make the use of this constraint clearer. The definition for |K| is entirely
equivalent to saying |K :: Maybe Int -> T Bool|.

%{
%format bu = "\beta"
%format bu1
We now wish to infer the type of
\[
|\x -> case x of { K n -> isNothing n }|
\]
where |isNothing :: forall a. Maybe a -> Bool| checks for an empty |Maybe|.
Consider any
mention of a new unification variable to be fresh. We assign |x| to have type
$[[au0]]$ and the result of the function to have type $[[bu0]]$. By the
existence of the constructor |K| in the |case|-match, we learn that $[[au0]]$
should really be $[[&&Tx{} au1_{}]]$. Inside the |case| alternative, we
now have a given constraint $[[&Bool{} [Type{}]~[Type{}] au1_{}]]$. We then
instantiate the polymorphic |isNothing| with a unification variable $[[bu1]]$,
so that the type of |isNothing| is |Maybe bu1 -> Bool|. We can now emit two equality
constraints:
\begin{itemize}
\item The argument type to |isNothing| (|Maybe bu1|) must match the type of |n| (|[Int]|).
\item The return
type of the |case| expression ($[[bu0]]$) is the return type of |isNothing| (|Bool|).
\end{itemize}
Pulling this all together, we get the following unification telescope:
\[
[[O]] =[
\begin{array}[t]{l}
[[au0 :Irrel forall{}. Type{}]],\\
[[bu0 :Irrel forall{}. Type{}]],\\
[[au1 :Irrel forall{}. Type{}]],\\
[[cu0 : forall{}. au0_{} [Type{}]~[Type{}] &&Tx{} au1_{}]],\\
[[bu1 :Irrel forall c : &Bool{} [Type{}]~[Type{}] au1_{}. Type{}]], \\
[[cu1 : forall c : &Bool{} [Type{}]~[Type{}] au1_{}. (&Maybe{} bu1_c [Type{}]~[Type{}] &Maybe{} &Int{})]], \\
[[cu2 : forall c : &Bool{} [Type{}]~[Type{}] au1_{}. (bu0_{} [Type{}]~[Type{}] &Bool{})]]
\end{array}
\]

Before we walk through what the solver does with such a telescope, what
\emph{should} it do? That is, what's the type of our original expression?
It turns out that this is not an easy question to answer! That expression
has no principal type. Both of the following are true:
\[
\begin{array}{l}
| (\x -> case x of { K n -> isNothing n }) :: forall a. T a -> a | \\
| (\x -> case x of { K n -> isNothing n }) :: forall a. T a -> Bool |
\end{array}
\]
%
%if style == newcode
\begin{code}
test1 = (\x -> case x of { K n -> isNothing n }) :: forall a. T a -> a
test2 = (\x -> case x of { K n -> isNothing n }) :: forall a. T a -> Bool
-- test3 = (\x -> case x of { K n -> isNothing n })
\end{code}
%endif
%
Note that neither |T a -> a| nor |T a -> Bool| is more general than the
other.

We would thus like the solver to fail when presented with this unification
telescope. This is true, even though there is a solution to the inference
problem (that is, a valid zonker $[[Z]]$ with a telescope of quantified
variables $[[D]]$; see the specification of $[[|->solv]]$, \pref{sec:solv-spec}):
\begin{align*}
%\begin{array}{rcl}
[[D]] &= [[&&ax :Irrel Type{}]] \\
[[Z]] &=
\begin{array}[t]{@@{}l}
[[forall{}. &&Tx{} &&ax/au0]], \\
[[forall{}. &Bool{}/bu0]], \\
[[forall{}. &&ax/au1]], \\
[[forall{}. <&&Tx{} &&ax>/cu0]], \\
[[forall c. &Int{}/bu1]], \\
[[forall c. <&Maybe{} &Int{}>/cu1]], \\
[[forall c. <&Bool{}>/cu2]]
\end{array}
%\end{array}
\end{align*}

The problem is that here is another valid substitution for $[[bu0]]$ and $[[cu2]]$:
\[
[[Z]] =
\begin{array}[t]{@@{}l}
 \ldots, \\
[[forall {}. &&ax/bu0]], \\
\ldots, \\
[[forall c. sym c/cu2]]
\end{array}
\]
%
These zonkers correspond to the overall type |T a -> Bool| and |T a -> a|, respectively.

We must thus ensure that the solver rejects $[[O]]$ outright. This is achieved
by making $[[bu0]]$ untouchable when considering solving the $[[cu2]]$ constraint.\footnote{Why this particular mechanism works is discussed in some depth
by \citet[Section 5.2]{outsidein}.}
As described by \citet[Section 5.5]{outsidein}, the solver considers the
constraints individually. When simplifying (\outsidein/'s terminology
for solving a simple, non-implication constraint) the $[[cu1]]$ and
$[[cu2]]$ constraints, any unification variable not quantified by $[[c]]$ is considered
untouchable.\footnote{To make this a bit more formal, I would need to label
the quantification by $[[c]]$ by some label drawn from an enumerable set
of labels. The touchable unification variables would be those quantified
by the same label as the constraint being simplified. We cannot just use
the name $[[c]]$, as names are fickle due to potential $\alpha$-variation.}
Thus, $[[bu0]]$ is untouchable when simplifying $[[cu2]]$, so the solver
will never set $[[bu0]]$ to anything at all. It will remain an ambiguous
variable and a type error will be issued. 

Contrast this with $[[au1]]$, which is also not set by the solver.
This variable, however, is fully unconstrained and can be quantified over
and turned into the non-unification variable $[[&&ax]]$.
There is no way to quantify over $[[bu0]]$, however.

Despite not setting $[[bu0]]$, the solver is free to set $[[bu1]]$ which
is considered touchable, as it is also quantified by $[[c]]$. The unification
variable $[[bu1]]$ is fully local to the |case| alternative body, and setting
it can have no effect outside of the |case| expression. In the terminology
of \outsidein/, that unification would be introduced by $\exists [[bu1]]$
in an implication constraint. In our example, the ability to set $[[bu1]]$
means that we get only one type error reported, not two.
%}

\section{Bidirectional typechecking}
\label{sec:bidirectional}
\label{sec:bidir-dependent-pattern-match}

Like previous algorithms for
GHC/Haskell~\cite{practical-type-inference,visible-type-application,gundry-thesis}, \bake/
takes a bidirectional approach~\cite{local-type-inference}. The fundamental
idea to bidirectional typechecking is that, sometimes, the type inference
algorithm knows what type to look for. When this happens, the algorithm
should take advantage of this knowledge.

Bidirectional typechecking works by defining two separate algorithms: a type
synthesis algorithm and a type checking algorithm. The former is used when
we have no information about the type of an expression, and the latter is
used when we do indeed know an expression's expected type. The algorithms
are mutually recursive because of function applications: knowing the result
type of a function call does not tell you about the type of the function
(meaning the checking algorithm must use synthesis on the function),
but once we know the function's type, we know the type of its arguments
(allowing the synthesis algorithm to use the more informative checking
algorithm).

Historically, bidirectional typechecking in Haskell has been most useful when
considering higher-rank polymorphism---for example, in a type
like |(forall a. a -> a) -> Int|. Motivating higher-rank types would bring
us too far afield, but the literature has helpful examples~\cite{practical-type-inference,visible-type-application} and there is a brief introduction in
\pref{sec:higher-rank-types}. Naturally, Dependent Haskell continues to
use bidirectional typechecking to allow for higher-rank types, but there
is now even more motivation for bidirectionality.

As discussed above (\pref{sec:untouchability}), bringing equality constraints
into scope makes some unification variables untouchable. In practice, this
means that the result type of a GADT pattern match must be known; programmers
must put type annotations on functions that perform a GADT pattern match.

In a dependently-typed language, however, \emph{any} pattern match might
bring equality constraints into scope, where the equality relates the scrutinee
with the pattern. For example, if I say something as simple as
|case b of { True -> x ; False -> y }|, I may want to use the fact that
|b ~ True| when typechecking |x| or |b ~ False| when typechecking |y|. This
is, of course, dependent pattern matching (\pref{sec:dependent-pattern-match}).
Our problem now is that it seems that \emph{every} pattern match introduces
an equality constraint, meaning that the basic type inference of Haskell might
no longer work.

The solution is to take advantage of the equality available by dependent
pattern matching only when the result type of the |case| expression is
being propagated downwards---that is, when the inference algorithm is
in checking mode. If we do not know a |case| expression's overall type,
then the pattern match is treated as a traditional, non-dependent pattern
match. Without bidirectional typechecking, the user might have to annotate
which kind of match is intended.\footnote{The Dependent Haskell described
by \citet{gundry-thesis} indeed has the user annotate this choice for
|case| expressions. Due to Gundry's restrictions on the availability of
terms in types (see his Section 6.2.3), however, the bidirectional approach would
have been inappropriate in his design.}

\subsection{Invisibility}

As discussed in \pref{sec:dep-haskell-vis}, Dependent Haskell programmers can
choose the visibility of their arguments: A visible argument must be provided
at every function call, while an invisible one may be elided. If the programmer
wants to write an explicit value to use for an invisible argument, prefixing
the argument with $\at$ allows it to stand for the invisible parameter.

In the context of type inference, though, we must be careful. As explored
in my prior work~\cite{visible-type-application}, invisible arguments are
sometimes introduced at the whim of the compiler. For example, consider
%
\begin{code}
-- |isShorterThan :: [a] -> [b] -> Bool|
isShorterThan xs ys = length xs < length ys
\end{code}
%
Note that the type signature is commented out. The function |isShorterThan|
takes two invisible arguments, |a|, and |b|. Which order should they appear
in? Without the type signature for guidance, it is, in general, impossible
to predict what order these will be generalized. See \citet[Section 3.1]{visible-type-application} for more discussion on this point.

Despite the existence of functions like |isShorterThan| with fully-inferred
type signatures, we wish to retain principal types in our type system---at
least in the subset of the language that does not work with equality
constraints.
We thus must have \emph{three} different levels of visibility:
\begin{description}
\item[Required] parameters (also called visible) must be provided at
function call sites.
\item[Specified] parameters are invisible, but their order is user-controlled.
These parameters are to functions with type signatures or with an explicit
|forall ...|.
\item[Inferred] parameters (called ``generalized'' in \citet{visible-type-application}) are ones invented by the type inference algorithm (like the parameter
|a| in the example used to explain untouchable variables;
see \pref{sec:untouchability}). They cannot ever be instantiated
explicitly. All coercion abstractions are inferred.
\end{description}
Note that these three levels of visibility are not a consequence of dependent
types, but of having an invisibility override mechanism; these three levels
of visibility are fully present in GHC 8. In the judgments that form \bake/,
I often write a subscript $[[Req]]$, $[[Spec]]$, or $[[Inf]]$to $\Pi$ symbols indicating the visibility of the
binders quantified over. These subscripts have no effect on well-formedness
of types and are completely absent from pure \pico/.

Following my prior work, both the synthesis
and checking algorithms are split into two judgments apiece: one written
$[[|->ty]]$ and one written $[[*|->ty]]$. The distinction is that the
latter works with types that may have invisible binders, while the former
does not. For example, a type produced by the $[[|->ty]]$ judgment in synthesis
mode is guaranteed not to have any invisible (that is, specified or
inferred) binders at the outermost level. Thus when synthesizing the type
of $[[_t1]]$ in the expression $[[_t1 _t2]]$, we use the $[[|->ty]]$ judgment,
as we want any invisible arguments to be inferred in preparation of applying
$[[_t1]]$ to $[[_t2]]$. Considering the algorithm in checking mode,
when processing a traditional $\lambda$-expression, we want the rule to be part of the $[[|->ty]]$ judgment,
to be sure that the algorithm has already skolemized (\pref{sec:skolemization})
the known type down to
one that accepts a visible argument. Conversely, the rule for an expression
like |\ ^^ (at a) -> ...| must belong in the $[[*|->ty]]$ judgment, as we want
to see the invisible binders in the type to match against the invisible argument
the programmer wishes to bind.

The interplay between the starred judgments and the unstarred nudges this
system toward principal types. Having these two different judgments is indeed
one of the main innovations in my prior work~\cite{visible-type-application},
where the separation is necessary to have principal types.

\subsection{Subsumption}
\label{sec:subsumption}

Certain expression forms do not allow inward propagation of a type. As mentioned
above, if we are checking an expression |f x| against a type |tau|, we have
no way of usefully propagating information about |tau| into |f| or |x|.
Instead, we use the synthesis judgment for |f| and then check |x|'s type
against the argument type found for |f|. After all of this, we will get
a type |tau'| for |f x|. We then must check |tau'| against |tau|---but
they do not have to match exactly. For example, if |tau'| is |forall a. a -> a|
and |tau| is |Int -> Int|, then we're fine, as any expression of the former
type can be used at the latter.

What we need here is a notion of \emph{subsumption}, whereby we say that
|forall a. a -> a| \emph{subsumes} |Int -> Int|, written
\[
|forall a. a -> a| [[<=]] |Int -> Int|
\]
For reasons well articulated in prior work~\cite[Section 4.6]{practical-type-inference}, my choice for the subsumption relation does \emph{deep skolemization}.
This means that the types |forall a. Int -> a -> a| and |Int -> forall a. a -> a|
are fully equivalent and is backward compatible with the current treatment
of non-prenex types in GHC.

\begin{figure}[t!]
\ottdefnIPrenexSimp{}\\
\ottdefnISubTwoSimp{}\\
\ottdefnISubSimp{}
\caption{Subsumption in \bake/ (simplified)}
\label{fig:subsumption}
\end{figure}

\bake/'s subsumption relation is in \pref{fig:subsumption}. The rules
in this figure are simplified from the full rules (which appear in
\pref{sec:app-subsumption}), omitting constraint generation and elaboration.
The rules in each judgment are meant to be understood as an algorithm,
trying earlier rules before later ones. Thus, for example, rule \rul{Sub\_Unify}
is not as universal as it appears.

The entry point is the bottom, unstarred subsumption judgment. It
computes the prenex form of $[[k2]]$ using the auxiliary judgment
$[[|->pre]]$ and instantiates $[[k1]]$. (The $[[Spec]]$ superscript to
$[[|->inst]]$ says to instantiate any argument that is no more visible
than $[[Spec]]$---that is, either $[[Inf]]$ or $[[Spec]]$ arguments.)
The instantiated $[[k1']]$ and prenexed $[[k2']]$ are then compared using
the starred subsumption judgment.\footnote{The stars on these judgments
have a different meaning than the star on $[[|->ty]]$; they are borrowed
from the notation by \citet{practical-type-inference},
not \citet{visible-type-application}.}

The starred judgment has the
usual contravariance rule for functions. This rule, however, has three
interesting characteristics.
\paragraph{Dependency}
We cannot simply compare $[[k2]] [[<=]] [[k4]]$. The problem is that
$[[k2]]$ has a variable $[[a]]$ of type $[[k1]]$ in scope, whereas
$[[k4]]$ has a variable $[[b]]$ of type $[[k3]]$ in scope. Contrast this
rule to a rule for non-dependent functions where no such bother arises.
In the fully detailed versions of these judgments, learning that
$[[k1 <= k2]]$ gives us a term $[[t]]$ such that
$[[t]] : [[UPI _:Rel k1. k2]]$---that is, a way of converting a $[[k1]]$ into
a $[[k2]]$. I include such a $[[t]]$ when checking whether $[[k3 <= k1]]$.
This $[[t]]$ is then used to convert $[[b]] : [[k3]]$ into a value of type
$[[k1]]$, suitable for substitution in for $[[a]]$. With this substitution
completed, we can perform the subsumption comparison against $[[k4]]$ as
desired.

\paragraph{Matchable functions subsume unmatchable ones}
Rule \rul{Sub\_Fun} includes a subsumptive relationship among
the two flavors of $\Pi$. Whenever an unmatchable $[[UPI]]$-type is expected,
surely a matchable $[[MPI]]$-type will do. Thus we allow either $\Pi$
on the left of the $[[<=]]$. Note that the other way would be wrong: not only
might an unmatchable $[[UPI]]$-type not work where a matchable $[[MPI]]$-type
is expected, but we also have no way of creating the $[[MPI]]$-type during
elaboration. Our need to elaborate correctly keeps us from getting this
wrong.

\paragraph{$[[Irrel]]$ subsumes $[[Rel]]$}
Finally, the rule also includes a subsumptive relationship among
relevances. If the relevances $[[rel1]]$ and $[[rel2]]$ match up, then
all is well. But also if $[[rel1]]$ is $[[Irrel]]$ and $[[rel2]]$ is $[[Rel]]$,
we are OK. If $[[rel2]]$ is $[[Rel]]$, that says that the expression we
are checking is allowed to use its argument relevantly, but nothing goes
wrong if the expression, in fact, does not (that is, if $[[rel1]]$ is
$[[Irrel]]$). Once again, elaboration keeps us honest here; if the rule
is written
the wrong way around, there is no sound way to elaborate.

\subsection{Skolemization}
\label{sec:skolemization}

In checking mode, the $[[*|->ty]]$ judgment \emph{skolemizes} any invisible
quantifiers in the known type.\footnote{I am following \citet{practical-type-inference} in my use of the word ``skolem''. I understand that this word may
have slightly different connotation in a logical context, but my use here
has become standard in the study of GHC/Haskell.}
As an example, consider
\[
|(\ x -> x) :: forall a. a -> a|
\]
When checking the $\lambda$-expression against that type, we first must
dispose of the |forall a|. This is done by essentially making |a| a 
fresh type constant, equal to no other. This act is called skolemization;
|a| becomes a skolem. The variable |x| is then given
this type |a|, and the body of the $\lambda$ indeed has type |a| as desired.

As we look at more complicated examples, a question arises about how deeply
to skolemize. Here is an illustrative example, taken from prior work~\cite{visible-type-application-extended}:
\begin{code}
x = \ 5 z -> z
 -- |x| is inferred to have type |forall a. Int -> a -> a|

y :: Int -> forall b. b -> b
y = x
\end{code}
In this example, we are checking |x| of type |forall a. Int -> a -> a|
against the type |Int -> forall b. b -> b|. We must be a bit careful here,
though: |x|'s type is fully inferred, and thus its quantification over |a|
is $[[Inf]]$, not $[[Spec]]$. With the right flags,\footnote{\texttt{-fprint-explicit-foralls}, specifically} GHC prints |x|'s type
as |forall {a}. Int -> a -> a| to denote that it is not available for a
visibility override.

The type we are checking against
does not have any invisible binders at the top (its first binder is the
visible one for |Int|), so we do not initially skolemize.
We instead discover that there is no checking rule for variables
and have to use the fall-through case for checking, which does
synthesis and then a subsumption check.
However, a naive approach would be wrong here: if we synthesize the
type of |x|, we will get the instantiated |Int -> alpha -> alpha|.
This is because $[[Inf]]$ binders are always instantiated immediately,
much like in the original syntax-directed version of the Hindley-Milner
type system~\cite{damas-thesis,mini-ml}. In the subsumption check, we will
want to set |alpha| to be |b|, the skolem created from |y|'s type
signature. We will be unable to do so, however, because doing so would
be ill-scoped: |alpha| occurs in the unification telescope before |b|
is ever brought into scope. This means that it would be ill-scoped for
the value chosen for |alpha| to refer to |b|.\footnote{Saying
that this example fails because of scoping is a vast improvement over
the state of affairs in \citet{visible-type-application-extended}, where a delicate
line of reasoning based on the subtleties of the Barendregt convention is
necessary to show how this example goes awry. By tracking our unification
variables in a telescope, problems like this become much clearer.}
It would  quite unfortunate to reject this example, because the subsumption
judgment, with its deep skolemization, would have this work out if only we
didn't instantiate that $[[Inf]]$ binder so eagerly.

Instead, I have written the \rul{ITyC\_Infer} rule (details in
\pref{sec:infer-rule}) to eagerly skolemize the known type deeply,
effectively \emph{before} ever looking at the expression. This puts
|b| firmly into scope when consider |alpha|, and the subsumption check
(and later solver) succeeds.

The solution to this problem proposed in prior work is to do deep skolemization
in the checking $[[*|->ty]]$ judgment. This works in the System SB
of \citet{visible-type-application}. However, it fails us here. The problem
is that Dependent Haskell allows for constructs like
|\ n (at a) -> ...|. If we check that expression against
|Int -> forall a. a -> a|, we want the |a|s to match up. Yet deeply skolemizing
the type we are checking against will eliminate the |a| and our algorithm
will reject the code. We thus instead do shallow skolemization in
$[[*|->ty]]$ and instead save the deep skolemization until we are forced
to switch into synthesis mode.

Returning to the |x|/|y| example, here is how it plays out:
\begin{enumerate}
\item The variable |x| is inferred to have type |forall {a}. Int -> a -> a|
when processing the declaration for |x|.
\item We then check the body of |y| against the type |Int -> forall b. b -> b|.
As there are no invisible binders, no skolemization happens right away.
\item We quickly find that no checking rules apply. We then deeply skolemize
the expected type, getting |Int -> b -> b| for a skolem |b|.
\item Now, we synthesize the type for the expression |x|, getting
|Int -> alpha -> alpha|.
\item The subsumption relation checks whether |Int -> alpha -> alpha|
subsumes |Int -> b -> b|. This is indeed true with |alpha := b|,
and the definition
for |y| is accepted.\footnote{Although not visible in the simplified
presentation of \rul{Sub\_DeepSkol}
in \pref{fig:subsumption},
it is critical that $[[k2]]$ is skolemized \emph{before} $[[k1]]$ is
instantiated, lest we end up with the same scoping problem. This can be
seen in the full rule (\pref{sec:app-subsumption}) with the fact that
we include $[[O1]]$ in the final generalization step. In contrast to
other potential pitfalls mentioned earlier, leaving $[[O1]]$ out of this
line does not imperil the soundness of elaboration; it is only a matter
of expressiveness of the source Haskell.}
\end{enumerate}

We have thus accepted our problem example and remain in line with the
declarative system proposed in my prior
work~\cite[Section 6.2]{visible-type-application}.

\section{Generalization}
\label{sec:generalizer}

There is one final aspect of the inference algorithm that requires study
before we look at the individual pieces: the generalization operation.\footnote{What I call generalization here is precisely what \citet[Section 7.5]{gundry-thesis} calls
``parameterisation'' and writes with $\nearrow$.} That said, in terms of understanding the \bake/ algorithm, having a strong grasp
on generalization is not terribly important; this is merely a technical
step needed to make the mathematics hold together.

Suppose we are synthesizing the type of a $\lambda$-expression |\x -> tau|.
We choose a unification variable $[[au]]$ for the type of |x|. We then must
put $[[&&xx :Rel au_{}]]$ into the context when synthesizing the type for |tau|.
Synthesizing this type will produce a unification telescope $[[O]]$. Now
we have a problem: what unification telescope will we return from synthesizing
the type of the entire $\lambda$-expression? It looks something like
$[[au :Irrel forall{}.Type{}, &&xx :Rel au_{}, O]]$ but, critically, that is not
a unification telescope, as that context contains a binding for an ordinary
\pico/ variable, |x|.

It might be tempting at this point simply to return a mixed telescope of
unification variables and \pico/ variables, and just to carry on. The problem
here is that we will lose track of the local scope of |x|. Perhaps something
later, outside of the $\lambda$-expression, will end up unifying with |x|---which
would be a disaster. No, we must get rid of it.

\begin{figure}
\ottdefnIIGen{}
\caption{\bake/'s generalization operation}
\label{fig:generalization}
\end{figure}

The solution is to generalize $[[O]]$ over |x|. This operation is written
$[[O +> &&xx :Rel Type{} ~> O';X]]$. (The mnemonic behind the choice of
$[[+>]]$ is that we are essentially moving the $[[&&xx :Rel Type{}]]$
binding to the right, past $[[O]]$.) The output unification telescope
$[[O']]$ binds the same unification variables as $[[O]]$, but each one
will be generalized with respect to |x|. The definition of this judgment
appears in \pref{fig:generalization}. The rules are a bit complicated
by the fact that we may generalize a unification variable binding multiple
times; both recursive rules thus assume a telescope $[[D']]$ that has
already been generalized.

The new construct $[[X]]$ is a \emph{generalizer}. It is a substitution-like
construct that maps unification variables to vectors, which you may recall
are lists of arguments $[[ps]]$. In this case, we simply use the domain
of $[[D]]$ as the vector, where my use of $[[dom(D)]]$ as a list of arguments
means to insert the irrelevance braces around irrelevantly-bound variables.
Generalizers are necessary because generalizing changes the type of
unification variables; we must then change the occurrences of them as well.

Generalizers operate like this:
\begin{definition*}[Generalizing {[\pref{defn:generalizer}]}]
A generalizer is applied postfix as a function. It operates homomorphically
on all recursive forms and as the identity operation on leaves other
than unification variables. Generalizing unification variables
is defined by these equations:
\[
\begin{array}{r@@{\quad}c@@{\quad}r@@{\;}l}
[[au |-> ps1 \in X]] & \Rightarrow & [[au_ps2[X] &= au_{{ps1,ps2}}]]\\
\text{otherwise} && [[au_ps[X] &= au_{{ps[X]}}]] \\
[[cu |-> ps1 \in X]] & \Rightarrow & [[cu_ps2[X] &= cu_{{ps1,ps2}}]]\\
\text{otherwise} && [[cu_ps[X] &= cu_{{ps[X]}}]]
\end{array}
\]
\end{definition*}
Just like the generalization judgment (\pref{fig:generalization}), the
generalization operation $[[ [X] ]]$ prepends the newly-generalized variables
to those already there.

\section{Type inference algorithm}

\begin{figure}[t!]
\[%\def\arraystretch{1.3}
\begin{array}{cl}
[[S;P |->ty _t ~> t : k -| O]] & \text{synthesize a type (no invis.~binders)} \\
[[S;P *|->ty _t ~> t : k -| O]] & \text{synthesize a type} \\
[[S;P |->ty _t : k ~> t -| O]] & \text{check a type (no invis.~binders)} \\
[[S;P *|->ty _t : k ~> t -| O]] & \text{check a type} \\
[[S;P |->pt _s ~> t -| O]] & \text{check a polytype (always with kind |Type|)} \\
[[S;P;rel *|->arg _t : k ~> p ; t -| O]] & \text{check an argument at relevance $[[rel]]$} \\
[[S;P;k0;t0 |->alt _alt : k ~> alt -| O]] & \text{check a case alt.~against an unknown type} \\
[[S;P;k0;t0 |->altc _alt : k ~> alt -| O]] & \text{check a case alt.~against a known type} \\
[[S;P |->q _qvar ~> a : k; vis -| O]] & \text{synth.~type of a bound var.} \\
[[S;P |->aq _aqvar ~> a : k -| O]] & \text{synth.~type of a bound var.~(w/o vis.~marker)} \\
[[S;P |->aq _aqvar : k ~> a : k'; x.t -| O]] & \text{check type of a bound var.~(w/o vis.~marker)} \\
[[|->pi quant ~> PI; rel]] & \text{interpret a quantifier} \\
[[|->fun k; rel1 ~> g; PI; a; rel2; k1; k2 -| O]] & \text{extract components of a function type} \\
[[S;P |->scrut _alts; k ~> g; D; H; ts -| O]] & \text{extract components of a scrutinee type} \\
[[|->inst_vis k ~> ps; k' -| O]] & \text{instantiate a type} \\
[[S;G |->decl _decl ~> x : k := t]] & \text{check a declaration} \\
[[S;G |->prog _prog ~> G'; theta]] & \text{check a program}
\end{array}
\]
\caption{\bake/ judgments}
\label{fig:bake-judgments}
\end{figure}

The schema of the judgments that define \bake/ appear in \pref{fig:bake-judgments}. I will not walk through each rule of each judgment to explain its inner
workings. As discussed in the introduction to this chapter, the individual
rules are largely predictable. They can be reviewed in their entirety in
\pref{app:inference-rules}, and the statements of lemmas that assert the
soundness of many of these judgments appear in \pref{sec:soundness-lemmas}.
Instead of a thorough review of the algorithm,
this section will call out individual
rules with interesting characteristics.

\subsection{Function application}

\begin{figure}[t!]
\begin{gather*}
\ottdruleITyXXApp{}\rulesep
\ottdruleITyXXAppSpec{}
\end{gather*}
\ottdefnIIFun{}
\ottdefnIIArg{}
\caption{Function applications in \bake/}
\label{fig:bake-app}
\end{figure}

As discussed above (\pref{sec:bidirectional}) function applications can
only synthesize their type. The two rules for synthesizing the type
of a function application (one for regular application and one for application
with $\at$) appear in \pref{fig:bake-app}, along with auxiliary judgments.

Walking through the \rul{ITy\_App} rule, we see that \bake/ first infers
the type $[[k0]]$ for the Haskell expression $[[_t1]]$, elaborating
$[[_t1]]$ to become $[[t1]]$ and producing a unification telescope $[[O1]]$.
The type for $[[t1]]$, though, might not manifestly be a function. This
would happen, for example, when inferring the type of |\x y -> x y|, where
the type initially assigned to |x| is just a unification variable $[[au]]$.
Instead of writing $[[k0]]$ as a function, \bake/ instead uses its
$[[|->fun]]$ judgment, which extracts out the component parts of a function
type.

It may be helpful in understanding the $[[|->fun]]$ judgment to see its
correctness property, as proved in \pref{sec:app-inference-soundness}:
\begin{lemma*}[Function position {[\pref{lem:ifun}]}]
If $[[S;P |=ty k : Type{}]]$ and $[[|->fun k; rel1 ~> g; PI; a; rel2; k1; k2 -| O]]$,
then $[[S;P,O |=co g : k [Type{}]~[Type{}] PI_Req a:rel2 k1.k2]]$.
\end{lemma*}
We can see here that $[[|->fun]]$ produces a coercion $[[g]]$ that relates
the input type $[[k]]$ to the output type $[[PI a:rel2 k1.k2]]$.
The input relevance $[[rel1]]$ is to be used as a default---\bake/ will
assume that a function uses its argument relevantly unless told otherwise.
Note that relevance of arguments is not denoted in the user-written source
code.

Looking at the definition of $[[|->fun]]$, we see two cases:
\begin{itemize}
\item If the input type $[[k]]$ is manifestly a $\Pi$-type, \bake/ just
returns its component pieces along with a reflexive coercion.
\item Otherwise, it invents fresh unification variables as emits a constraint
relating this variables to the input.
\end{itemize}
It might be tempting to define $[[|->fun]]$ only by the second rule,
\rul{IFun\_Cast}, but this would greatly weaken \bake/'s power. Doing so
would mean that the bidirectional algorithm would never be able to take
advantage of knowing a function's argument type. Furthermore, note that
$[[bu2]]$, the result type of the function in \rul{IFun\_Cast}, is not
generalized with respect to $[[a :rel bu1_{}]]$; a function type inferred
via \rul{IFun\_Cast} will surely be non-dependent. This decision was made
in keeping with the guiding principle that only simple types should be
inferred.

Once we have extracted the component parts of the function type, we can
check the argument with the $[[*|->arg]]$ judgment. This judgment takes
the relevance of the argument as an input; it simply uses the $[[*|->ty]]$
checking judgment and insert braces as appropriate.

Contrast the behavior of \rul{ITy\_App} to that of \rul{ITy\_AppSpec},
which, crucially, does not use $[[|->fun]]$. Consider what would happen if
the function's type is not manifestly a $\Pi$-type. We could, like in
\rul{IFun\_Cast} invent unification variables and emit a constraint. But
this would mean that the argument is \emph{inferred}, not \emph{specified}.
Using an inferred argument with a visibility override violates the
inference principles set forth by \citet{visible-type-application} and
would surely eliminate the possibility of principal types. Accordingly,
\rul{ITy\_AppSpec} avoids such behavior and simply looks to make sure
that the function's type is of the appropriate shape. If it is not,
\bake/ issues an error.

\subsection{Mediating between checking and synthesis}
\label{sec:infer-rule}

The two modes of \bake/ meet head-on when we are checking an expression
(such as a function application) that has no rules in the checking judgment.
The fall-through case of the checking judgment is this rule:
\[
\ottdruleITyCXXInfer{}
\]
We are checking that $[[_t]]$ has type $[[k2]]$. First, \bake/ synthesizes
$[[_t]]$'s type $[[k1]]$, producing unification telescope $[[O]]$.
We then must, as described in \pref{sec:skolemization}, deeply skolemize
$[[k2]]$. Pulling out the quantifiers in $[[k2]]$ (according to the
$[[|->pre]]$ judgment) gives us $[[UPI D.k2']]$.
We then \emph{generalize} $[[O]]$ by $[[D]]$. It is this generalization
step that allows the solver to solve unification variables in $[[O]]$
with skolems in $[[D]]$ and allows the example from \pref{sec:skolemization}
to be accepted. Having generalized, we then do the subsumption check.
We now must generalize $[[O2]]$, the output unification telescope from
the subsumption check, as $[[O2]]$ might refer to skolems bound in $[[D]]$.

Once again, the key interesting part of this rule is the first generalization
step. It is not necessary to do this in order to get correct elaboration,
but the analysis in my prior work~\cite[end of Section 6.1]{visible-type-application} suggests
that this is necessary in order to have principal types.

\subsection{|case| expressions}

We see in \pref{fig:bake-judgments} that there are two judgments for checking
|case| alternatives. These correspond to the two rules for checking |case|
expressions, one for synthesis (\rul{ITy\_Case})
and one for checking (\rul{ITyC\_Case}). I refrain from including
the actual rules here, as their myriad and ornate details would be
distracting; the overly-curious can see \pref{app:inference-rules} for these
details.

As discussed previously (Sections \ref{sec:dependent-pattern-match} and
\ref{sec:bidirectional}), a |case| expression is treated differently depending
on whether we can know its result type. In the case where we do not
(\rul{ITy\_Case}), \bake/ invents a new unification variable $[[bu]]$ for
the result type and checks each case alternative against it. This is why
the $[[|->alt]]$ judgment takes a result type, even though it is used
during synthesis. After all, we do require all alternatives to produce the
\emph{same} result type. Producing the unification variable within each
alternative would risk running into a skolem escape, whereby the result type
might mention a variable locally bound within the alternative. It is simpler
just to propagate the $[[bu]]$ down into $[[|->alt]]$. The $[[|->alt]]$ judgment,
in turn, does not use the equality gotten from dependent pattern matching
when checking alternatives. Recall that doing so during synthesis mode
would cause trouble
because the equality assumption would make the $[[bu]]$ unification variable
untouchable when solving constraints emitted while processing the alternatives.

On the other hand, the $[[|->altc]]$ judgment is used from \rul{ITyC\_Case},
in checking mode. This judgment is almost identical to $[[|->alt]]$ except
that it allows the alternatives to make use of the dependent-pattern-match
equality.

\subsection{Checking $\lambda$-expressions}
\label{sec:annotated-lambdas}

Consider checking this expression:
\begin{equation} \label{example:non-dep}
| (\ (f :: Int -> Int) -> f 5) :: (forall a. a -> a) -> Int |
\end{equation}
%if style == newcode
\begin{code}
test8 = (\ (f :: Int -> Int) -> f 5) :: (forall a. a -> a) -> Int
-- test9 = (\ (f :: forall a. a -> a) -> f 5) :: (Int -> Int) -> Int
\end{code}
%endif
This expression should be accepted. The $\lambda$ takes a function
over |Int|s and applies it. The type signature then says that the
$\lambda$ should actually be applicable to any polymorphic endofunction.
Of course, such a function can be specialized to |Int|, so all is well.
Indeed, the expression above is accepted by GHC.

The example above, however, is not dependent. Surprisingly, the intuition
in the above paragraph does not generalize to the dependent case.
Consider this (contrived) example:
\begin{equation} \label{example:dep}
| (\ (f :: Bool -> Bool) -> P) :: pi (g :: forall a. a -> a) -> Proxy !(g 5, g !True) |
\end{equation}
where we have
\begin{code}
data Proxy :: forall k. k -> Type where
  P :: forall k (a :: k). Proxy a
-- equivalent to |data Proxy a = P|
\end{code}
Once again, the annotation on the $\lambda$ argument is a specialized version
of the argument's type as given in the type signature. And yet, this expression
must be rejected.

One way to boil this problem down is to consider what type we check the expression
|P| against. When we are checking |P|, we clearly have |f :: Bool -> Bool|
in scope. Yet the natural type to check |P| against is |Proxy !(g 5, g !True)|,
which mentions |g|, not |f|. Even if the names were to be fixed, we would
still have the problem that |g 5| is certainly not well-typed if
|g| has type |Bool -> Bool|. We are stuck.

Another way to see this problem is to think about elaborating the subsumption
judgment. In \pref{example:non-dep}, type inference will check whether
$|forall a. a -> a| [[<=]] |Int -> Int|$. When it discovers that this is
true, the subsumption algorithm will also produce a function that takes something
of type |forall a. a -> a| to something of type |Int -> Int|. If the
expression in \pref{example:non-dep} is applied to an argument 
(naturally, of type |forall a. a -> a|), then this conversion function readies
the argument to pass to the $\lambda$-expression.

In \pref{example:dep}, however, we need conversions both ways. We still need
the conversion from |forall a. a -> a| to |Bool -> Bool|, for exactly
the same reason that we need it for \pref{example:non-dep}. We also need
the conversion in the other direction (in this case, the impossible
conversion from |Bool -> Bool| to |forall a. a -> a|) when checking that
|P|, with |f :: Bool -> Bool| in scope, has type |Proxy !(g 5, g !True)|,
using |g :: forall a. a -> a|.

The solution to this is to have two separate rules, one in the non-dependent
case and one in the dependent case. \Bake/ looks at the type being checked
against (let's call it $[[t]]$).
If $[[t]]$ uses its argument dependently, then \bake/ requires that
the annotation on the $\lambda$ argument and the function type as found
in $[[t]]$ can be proved equal---that is, that there is a coercion between
them. Otherwise, we use subsumption, just as in \pref{example:non-dep}.
You can view the two rules in \pref{sec:checking-judgments}; as usual,
the rules are a bit cluttered to present here.

\section{Program elaboration}
\label{sec:idecl}

\begin{figure}[t!]
\ottdefnIIDecl{}\\
\ottdefnIIProg{}
\caption{Elaborating declarations and programs}
\label{fig:prog-decl-elab}
\end{figure}

Up until now, this chapter has focused more on the gate-keeping services
provided by \bake/, preventing ill-formed programs from being accepted.
In this section, we will discuss elaboration, the process of creating the
\pico/ program that corresponds to an input Haskell program. Let's look
in particular on the highest levels of elaboration, processing Haskell
declarations and programs. See \pref{fig:prog-decl-elab} for the two judgments
of interest.

\subsection{Declarations}
The $[[|->decl]]$ judgment processes the two forms of declaration included
in the Haskell subset formalized here: unannotated variable declarations
and annotated variable declarations. It outputs the name of the new
variable, its type $[[k]]$ and its value $[[t]]$. Note that the environment
used in $[[|->decl]]$ is $[[S]];[[G]]$, with a context containing only
\pico/ variables, no unification variables. These are top-level declarations
only.

Rule \rul{IDecl\_Synthesize} simply ties together the pieces of using the
synthesis judgment and the solver. Note that the definitions of
$[[t']]$ and $[[k']]$ in the rule generalize over the telescope $[[D]]$
produced by the solver, and that the $\Pi$-type formed marks the
binders as inferred, never specified.

Rule \rul{IDecl\_Check} is a bit more involved. It first must check the
type signature using the $[[|->pt]]$ judgment,
to make sure $[[_s]]$ it is a well-formed polytype.
This process might emit constraints, and we must solve these before tackling
the term-level expression. This would happen, for example, in the type
|forall a. Proxy a -> ()|, as |a|'s kind is unspecified. The solver may
produce a telescope $[[D1]]$ to generalize by. In our example, this
telescope would include $[[&&kx :Irrel Type{}]]$, the type of |a|.
Having sorted out the type signature, we can now proceed to the expression
$[[_t]]$, which is checked against $[[s']]$ the generalized
\pico/ translation of the user-written polytype $[[_s]]$. We must solve
once again. In this invocation of the solver, we insist that no further
generalization be done because the user has already written the entire
type of the expression. This decision is in keeping with standard Haskell,
where a declaration like
\begin{spec}
bad :: a -> String
bad x = show x
\end{spec}
is rejected, because accepting the function body requires generalizing over
an extra |Show a| constraint.

\subsection{Programs}
The elaboration of whole programs is generally straightforward. This 
algorithm
appears in \pref{fig:prog-decl-elab}. The judgment
$[[S;G |->prog _prog ~> G'; theta]]$ produces as output an extension to
the context, $[[G']]$, as well as a closing substitution $[[theta]]$
which maps the newly bound variable to its definition. (Recall that
this formalization of \bake/ ignores recursion; thus no variable
can be mentioned in its own declaration.)

The one non-trivial rule,
\rul{IProg\_Decl}, checks a declaration and then incorporates this declaration
into the context $[[G]]$ used to check later declarations.
There is one small twist here, though: because declared variables can be
used in types as well as in terms, we wish the typing context to remember
the equality between the variable and its definition. This is done via
the coercion variable $[[c]]$ included in the context in the second premise
to \rul{IProg\_Decl}.

\section{Metatheory}
\label{sec:bake-metatheory}

This chapter has explained the \bake/ algorithm in some detail, but what
theoretical properties does it have? A type inference algorithm is often
checked for soundness and completeness against a specification. However, as
argued by \citet[Section 6.3]{outsidein}, lining up an algorithm such as
\bake/ against a declarative specification is a challenge. Instead of writing
a separate, non-algorithmic form of \bake/, I present three results in this
section:
\begin{itemize}
\item I prove that the elaborated \pico/ program produced by \bake/ is
indeed a well-typed \pico/ program. This result---which I call soundness---marks
an upper limit on the set of programs that \bake/ accepts. If it cannot
be typed in \pico/, \bake/ must reject.\footnote{I do not prove a correspondence
between the Haskell program and the \pico/ program produced by elaboration.
It would thus theoretically be possible to design \bake/ to accept
all input texts and produce a trivial elaborated program. But that wouldn't
be nearly as much fun, and I have not done so.}
\item In two separate subsections,
I argue that \bake/ is a conservative extension both
of the \outsidein/
algorithm and the SB algorithm of \citet{visible-type-application}.
That is, if \outsidein/ or SB accepts a program, so does \bake/.
This results suggests that a version of GHC
based on \bake/ will accept all Haskell programs currently accepted.
These arguments---I dare not quite call them proofs---are stated in less
formal terms than other proofs in this dissertation. While it is likely
possible
to work out the details fully, the presentation of the other systems and
of {\bake/}/{\pico/} differ enough that the translation between the systems
would be fiddly, and artifacts of the translation would obscure the main
point. The individual differences are discussed below.

These conservativity results provide a lower bound on the power of \bake/,
declaring that some set of Haskell programs must be accepted by the algorithm.
\end{itemize}

The results listed above bound the power of the algorithm both from below
and from above, serving roughly as soundness and completeness results.
It is left as future work to define a precise specification of \bake/ and
prove that it meets the specification.

\subsection{Soundness}

Here is the fundamental soundness result:
\begin{theorem*}[Soundness of \bake/ elaboration {[\pref{thm:iprog}]}]
If $[[S |-ctx G]]$ and $[[S;G |->prog _prog ~> G'; theta]]$, then:
\begin{enumerate}
\item $[[S |-ctx G,G']]$
\item $[[S;G |-subst theta : G']]$
\item $[[dom(_prog) \subseteq dom(G')]]$
\end{enumerate}
\end{theorem*}
This theorem assumes that the starting environment is well-formed
$[[S |-ctx G]]$ and that \bake/ accepts the source language program
$[[_prog]]$. In return, the theorem claims that the context extension
$[[G']]$ is well-formed (assuming it is appended after $[[G]]$),
that the substitution $[[theta]]$ is a valid closing substitution (see below),
and that indeed the new context $[[G']]$ binds the variables declared in
$[[_prog]]$.

\begin{figure}
\ottdefnSubst{}
\caption{Validity of closing substitutions}
\label{fig:closing-subst}
\end{figure}

Closing substitutions are recognized by the new judgment $[[|-subst]]$,
which appears in \pref{fig:closing-subst}.
(Note the turnstile $[[|-]]$; this is a pure \pico/ judgment with no
unification variables in sight.) It uses a new notation $[[theta \inter zs]]$
which restricts the domain of a substitution $[[theta]]$ to operate only
on the variables $[[zs]]$. Informally, $[[S;G |-subst theta : D]]$
holds when the substitution $[[theta]]$ eliminates the appearance
of any of the variables in $[[D]]$. Here is the key lemma that asserts
the correctness of the judgment:

\begin{lemma*}[Closing substitution {[\pref{lem:closing-subst}]}]
If $[[S;G |-subst theta : D]]$ and $[[S;G,D,G' |- J]]$, then
$[[S;G,G'[theta \inter dom(D)] |- J[theta \inter dom(D)] ]]$.
\end{lemma*}

Here, I use a notation where $[[J]]$ stands for a judgment chosen from
$[[|-ty]]$, $[[|-co]]$, $[[|-prop]]$, $[[|-alt]]$, $[[|-vec]]$, $[[|-ctx]]$,
or $[[|-s]]$.

The use of $[[|-subst]]$ in the conclusion of the elaboration soundness
theorem means that the variable values stored in $[[theta]]$ actually
have the types as given in $[[G']]$.

Naturally, proving this theorem requires proving the soundness of all
the individual judgments that form \bake/. This proofs all appear
in \pref{sec:app-inference-soundness}.

\subsubsection{Adapting lemmas on $[[|-]]$ to $[[|=]]$}

The first step in establishing the soundness result is to ensure that
the structural lemmas proved for $[[|-]]$ judgments still hold over
the $[[|=]]$ judgments. While doing this for the definitions as given
does not pose a challenge, it is in getting these proofs to work
that all of the complications around unification variables (to wit,
zonkers and generalizers) arise.

Relating the two sets of judgments is accomplished by this key lemma:
\begin{lemma*}[Extension {[\pref{lem:extension}]}]
$[[S;G |- J]]$ if and only if $[[S;G |= J]]$.
\end{lemma*}
Note that the context must contain only \pico/ variables, never unification
variables. This fact is what allows the larger $[[S;G |= J]]$ to imply
the smaller $[[S;G |- J]]$.

\subsubsection{Soundness of the solver}
\label{sec:solver-soundness}

\begin{figure}
\ottdefnZonk{}
\caption{Zonker validity}
\label{fig:zonker-judgment}
\end{figure}

The solver $[[S;P |->solv O ~> D; Z]]$ produces a generalization telescope
and a zonker. In order to define a correctness property for this solver,
we first need a judgment that asserts the validity of the zonker. This
judgment appears in \pref{fig:zonker-judgment}. The judgment is quite
similar to the judgment classifying closing substitutions ($[[|-subst]]$,
in \pref{fig:closing-subst}), but it deals also with the complexity
of having unification variables quantified over telescopes.

Naturally, we must require that the solver produce a valid zonker.
We also require that the zonker be idempotent, as that is a necessary
requirement to prove the zonking lemma, below. Here is the soundness
property we are assuming of the solver. Note that this property is
the \emph{only} one we need to prove soundness of elaboration.

\begin{property*}[Solver is sound {[\pref{prop:isolv}]}]
If $[[S |=ctx P,O]]$ and
$[[S;P |->solv O ~> D; Z]]$,
then $[[Z]]$ is idempotent, $[[S |=ctx P,D]]$, and $[[S;P,D |=z Z : O]]$.
\end{property*}

\begin{lemma*}[Zonking {[\pref{lem:zonking}]}]
If $[[Z]]$ is idempotent, $[[S;P |=z Z : O]]$, and $[[S;P,O,D |= J]]$, then
$[[S;P,D[Z] |= J[Z] ]]$.
\end{lemma*}

\subsubsection{Soundness of generalization}

The following lemma asserts the correctness of the generalization judgment:
\begin{lemma*}[Generalization {[\pref{lem:igen}]}]
If $[[O +> D ~> O'; X]]$ and $[[S;P,D,O |= J]]$,
then $[[S;P,O',D |= J[X] ]]$.
\end{lemma*}

The proof of this lemma relies on the following smaller lemma (and its
counterpart for coercion variables):
\begin{lemma*}[Generalization by type variable {[\pref{lem:igen-tyvar}]}]
If $[[S;P,D,au :rel forall D'.k,P' |= J]]$,
then
$[[S;P,au :rel forall D,D'.k,D,P'[au |-> dom(D)] |= J[au |-> dom(D)] ]]$.
\end{lemma*}

\subsubsection{Soundness lemmas for individual judgments}
\label{sec:soundness-lemmas}

\begin{lemma*}[Instantiation {[\pref{lem:iinst}]}]
If $[[S;P |=ty t : k]]$ and $[[|->inst_vis k ~> ps; k' -| O]]$,
then $[[S;P,O |=ty t ps : k']]$ and $[[k']]$ is not a $\Pi$-type
with a binder (with visibility $[[vis2]]$) such that $[[vis2 <= vis]]$.
\end{lemma*}

\begin{lemma*}[Scrutinee position {[\pref{lem:iscrut}]}]
If $[[S;P |=ty t : k]]$ and $[[S;P |->scrut _alts; k ~> g; D; H'; ts -| O]]$,
then $[[S;P,O |=ty t |> g : MPI D. H'{} ts]]$ and
$[[S;Rel(P,O) |=ty H'{} ts : Type{}]]$.
\end{lemma*}

\begin{lemma*}[Prenex {[\pref{lem:ipre}]}]
If $[[S;Rel(P) |=ty k : Type{}]]$ and
$[[|->pre k ~> D; k'; t]]$, then
$[[S;P |=ty t : UPI x:Rel (UPI D. k'). k]]$.
\end{lemma*}

\begin{lemma*}[Subsumption {[\pref{lem:isub}]}] ~
Assume $[[S;Rel(P) |=ty k1 : Type{}]]$ and $[[S;Rel(P) |=ty k2 : Type{}]]$.
If either
\begin{enumerate}
\item $[[k1 *<= k2 ~> t -| O]]$, OR
\item $[[k1 <= k2 ~> t -| O]]$
\end{enumerate}
Then $[[S;P,O |=ty t : UPI x:Rel k1. k2]]$.
\end{lemma*}

\begin{lemma*}[Type elaboration is sound {[\pref{lem:isound}]}] ~
\begin{enumerate}
\item If any of the following:
\begin{enumerate}
\item $[[S |=ctx P]]$ and $[[S;P |->ty _t ~> t : k -| O]]$, OR
\item $[[S |=ctx P]]$ and $[[S;P *|->ty _t ~> t : k -| O]]$, OR
\item $[[S;Rel(P) |=ty k : Type{}]]$ and $[[S;P |->ty _t : k ~> t -| O]]$, OR
\item $[[S;Rel(P) |=ty k : Type{}]]$ and $[[S;P *|->ty _t : k ~> t -| O]]$
\end{enumerate}
Then $[[S;P,O |=ty t : k]]$.
\item
If $[[S |=ctx P]]$ and $[[S;P |->pt _s ~> s -| O]]$,
then $[[S;Rel(P,O) |=ty s : Type{}]]$.
\item
If $[[S;P |=ty t1 : PI_vis a:rel k1.k2]]$
and $[[S;P;rel *|->arg _t2 : k1 ~> p2; t2 -| O]]$,
then $[[S;P,O |=ty t1 p2 : k2[t2/a] ]]$.
\item
If $[[S;Rel(P) |=ty k : Type{}]]$,
$[[S;P |=ty t0 : MPI D. H{} ts]]$,
$[[S;Rel(P) |=ty H{} ts : Type{}]]$, and
$[[S;P;MPI D.H{} ts;t0 |->alt _alt : k ~> alt -| O]]$, then
$[[S;P,O;MPI D.H{} ts;t0 |=alt alt : k]]$.
\item
If $[[S;Rel(P) |=ty k : Type{}]]$,
$[[S;P |=ty t0 : MPI D. H{} ts]]$, 
$[[S;Rel(P) |=ty H{} ts : Type{}]]$,
and
$[[S;P;k0;t0 |->altc _alt : k ~> alt -| O]]$, then
$[[S;P,O;k0;t0 |=alt alt : k]]$.
\item
If $[[S |=ctx P]]$ and $[[S;P |->q _qvar ~> a : k; vis -| O]]$,
then $[[S;Rel(P,O) |=ty k : Type{}]]$.
\item
If $[[S |=ctx P]]$ and $[[S;P |->aq _aqvar ~> a : k -| O]]$,
then $[[S;Rel(P,O) |=ty k : Type{}]]$.
\item
If $[[S;P |=ty t0 : k]]$ and $[[S;P |->aq _aqvar : k ~> a : k'; x.t -| O]]$,
then $[[S;P,O |=ty t[t0/x] : k']]$.
\end{enumerate}
\end{lemma*}

\subsection{Conservativity with respect to \outsidein/}
\label{sec:oi}

I do not endeavor to give a full accounting of the \outsidein/ algorithm
here, instead referring readers to the original~\cite{outsidein}. I will
briefly explain judgments, etc., as they appear and refer readers to Figure
numbers from the original text.

\begin{table}
\begin{center}
\begin{tabular}{@@{}lccl@@{}}
\multicolumn{2}{c}{\outsidein/ construct} & \pico/ form & Notes \\ \hline
Axiom scheme & $[[QQ]]$ & $[[G]]$ & 
\begin{minipage}[t]{.45\textwidth}
\setlength{\baselineskip}{.8\baselineskip}
instances, etc.; implications are functions;
type family instances are via unfoldings \\[-1ex]
\end{minipage} \\
Given constraint & $[[Qg]]$, $[[Qr]]$ & $[[D]]$ & constraints are named in \pico/ \\
Wanted constraint & $[[Qw]]$ & $[[O]]$ & we must separate wanteds \& givens
\end{tabular}
\end{center}
\caption{Translation from \outsidein/ to \pico/}
\label{tab:oi-encode}
\end{table}

There are several mismatches between concepts in \outsidein/ and in
\pico/. Chief among these is that
\outsidein/ does not track unification variables in any
detail. All unification variables (and type variables, in general) in
\outsidein/ have kind $[[Type]]$, and thus there is no need for dependency
tracking. In effect, many judgments in \outsidein/ are parameterized by
an unwritten set of in-scope unification variables. We have no such luxury
of concision available in \bake/, and so there must be consideration
given to tracking the unification variables.

To partly bridge the gap between \outsidein/ and \bake/,
I define $[[encode]]$ which does the translation,
according to \pref{tab:oi-encode}. $[[encode]]$ing a construct from the left
column results in a member of the syntactic class
depicted in the middle column.

\outsidein/ differentiates
between algorithm-generated constraints $C$ and user-written ones
$Q$; the former contain implication constraints. I do not discern between
these classes, considering implication constraints simply as functions.
I will use $Q$ metavariables in place of \outsidein/'s $C$.\footnote{This
conflation of $Q$ and $C$ does not mean that Dependent Haskell is now
required to implement implication constraints; it would be easy to add
a post-typechecking pass (a ``validity'' check, in the vocabulary of the
GHC implementation) that ensures that no constraints have implications.}

A further difference between \outsidein/ and \bake/ is that the latter
is bidirectional. When \outsidein/ knows the type which it wishes to
assign to a term, it synthesizes the term's type and then emits an
equality constraint. In the comparison between the systems, we will
pretend that \bake/'s checking judgments do the same.

The fact that I must change my judgments does not imperil the practical
impact of the conservativity result---namely, programs that GHC accepts
today will still be accepted tomorrow. GHC already uses bidirectional
typechecking and so has already obviated the unidirectional aspect of
\outsidein/. However, in order to make a formal comparisons between that
published algorithm, it is helpful to restrict ourselves to a
unidirectional viewpoint.

A final difference is that \bake/ does elaboration, while \outsidein/
does not. I shall use the symbol $[[\cdot]]$ to denote an elaborated type
that is inconsequential in this comparison.

\subsubsection{Expressions}

\begin{claim*}[Expressions {[\pref{claim:oi-expr}]}]
If $[[G oi|-> _t : k ~> Qw]]$ under axiom set $[[QQ]]$ and signature
$[[S]]$, then
$[[S;G,encode(QQ) |->ty _t ~> \cdot : k -| aus :Irrel forall{}.Type{}, encode(Qw)]]$
where $[[aus = fuv(k) \union fuv(Qw)]]$.
\end{claim*}

This claim relates \outsidein/'s $[[G oi|-> _t : k ~> Qw]]$ judgment
(Figures 6 and 13 from \citet{outsidein}) to \bake/'s synthesis $[[|->ty]]$
judgment. Note that the output $[[O]]$ from \bake/'s judgment must include
both the wanteds ($[[encode(Qw)]]$) and also any unification variables
required during synthesis ($[[aus]]$).

To argue this claim, we examine the different rules that make up
\outsidein/'s judgment, using structural induction. The details appear
in \pref{sec:app-oi-expr}.

\subsubsection{The solver}

\begin{property*}[Solver]
If $[[QQ; Qg; aus1 oi|->solv Qw ~> Qr; Z]]$ where $[[S]]$ and $[[G]]$ capture
the signature and typing context for the elements of that judgment,
then $[[S;G, encode(QQ), encode(Qg) |->solv aus1 :Irrel forall{}.Type{}, encode(Qw) ~> as2:Irrel Type{}, encode(Qr)[as2/aus2]; as2/aus2, Z]]$,
where the $[[as2]]$ are fresh replacements for the $[[aus2]]$ which are free
in $[[Qr]]$ or unconstrained variables in $[[aus1]]$.
\end{property*}

This property is a bit more involved than we would hope, but all of the complication
deals with \bake/'s requirement of tracking unification variables more carefully
than does \outsidein/.
Underneath all of the faffing about with unification variables, the key
point here is that \bake/'s solver will produce the same residual constraint
$[[Qr]]$ as \outsidein/'s and the same zonking substitution $[[Z]]$.

I do not try to argue this property directly, as I do not present the
implementation for the solver. However, this property shows a natural
generalization of the solver in an environment that includes dependencies
among variables. Indeed, GHC's implementation of the solver already handles
such dependency.

\subsubsection{Programs}

\begin{claim*}[\rul{Bind}]
\label{lem:oi-bind}
If $[[G oi|-> _t : k ~> Qw]]$ and
$[[QQ; empt; fuv(k) \union fuv(Qw) oi|->solv Qw ~> Qr; Z]]$,
then
$[[S;G, encode(QQ) |->decl x := _t ~> x : UPI_Inf as :Irrel Type{}. (UPI_Inf encode(Qr). k[Z])[as/aus] := t]]$ for some $[[t]]$, where $[[aus = fuv(k[Z]) \union fuv(Qr)]]$ and $[[as]]$ are fresh replacements for the $[[aus]]$.
\end{claim*}

This claim relates \outsidein/'s \rul{Bind} rule (Figure 12) to \bake/'s
\rul{IDecl\_Synthesize} rule. It is a consequence of the claim on expressions
and the property above of the solver.

%% \begin{proof}
%% We will use \rul{IDecl\_Synthesize}.
%% \pref{lem:oi-expr} tells us
%% $[[S;G, encode(QQ) |->ty _t ~> t : k -| aus1:Irrel forall{}.Type{}, encode(Qw)]]$
%% where $[[aus1 = fuv(k) \union fuv(Qw)]]$.
%% Then \pref{prop:oi-solv} tells us
%% $[[S;G, encode(QQ) |->solv aus1 :Irrel forall{}.Type{}, encode(Qw) ~> as2:Irrel Type{}, encode(Qr)[as2/aus2]; as2/aus2, Z]]$, where the $[[as2]]$ are fresh replacements for the $[[aus2]]$ which are the free
%% in $[[Qr]]$ or unconstrained variables in $[[aus1]]$. The latter set is
%% precisely $[[fuv(k[Z])]]$.
%% \rul{IDecl\_Synthesize} then tells us
%% $[[S;G, encode(QQ) |->decl x := _t ~> x : UPI_Inf as2:Irrel Type{}, encode(Qr)[as2/aus2]. (k[Z]) := t']]$
%% for some $[[t']]$ (whose details are irrelevant).
%% We are done.
%% \end{proof}

\begin{claim*}[Conservativity over \outsidein/]
If $[[QQ; G oi|-> _prog]]$, $[[_prog]]$ contains no annotated bindings,
and $[[S]]$ captures the signature of the
environment $[[_prog]]$ is checked in,
then $[[S; G, encode(QQ) |->prog _prog ~> G'; theta]]$.
\end{claim*}

This claim relates the overall action of the \outsidein/ algorithm
(Figure 12) to \bake/'s algorithm for checking programs. It follows
directly from the previous claim.

Because of this, I believe that any program without top-level annotations
accepted by \outsidein/ is also accepted by \bake/.

\subsection{Conservativity with respect to System SB}
\label{sec:sb}

Here, I compare \bake/ with the bidirectional algorithm (called SB)
in Figure 8 of
\citet{visible-type-application}. That algorithm is proven to be
a conservative extension both of Hindley-Milner inference and also
of the bidirectional algorithm presented by \citet{practical-type-inference}.
This SB algorithm, along with \outsidein/,
 is part of the basis for the algorithm currently
implemented in GHC 8. 

Before we can successfully relate these systems, we must tweak both a bit
to bring their approaches more in line with one another:
\begin{itemize}
\item System SB assumes an ability to guess monotypes. This is evident,
for example, in the \rul{SB\_Abs} rule, where an unannotated $\lambda$-expression
is processed and the monotype of the argument is guessed. \Bake/, of course,
uses unification variables. I thus modify System SB to always guess
a unification variable when it guesses. The modified rules are
\rul{SB\_Abs}, \rul{SB\_InstS}, and \rul{SB\_Var}.
\item Because of the previous change, it is now unfair in
rule \rul{SB\_App} to insist that the result of synthesis be a function
type. Instead, the result of synthesizing the type of $e_1$ is an arbitrary
monotype, and the $[[|->fun]]$ judgment is used to expand this out to a
proper function type. Note that we do \emph{not} make a similar change
in \rul{SB\_TApp}; doing so would be tantamount to saying that a unification
variable might unify with a type with an invisible binder, something we
have forbidden. (See \pref{sec:solver-properties-impredicativity}.) We similarly must
modify \rul{SB\_DAbs} to allow for the possibility of a unification variable
being checked against.
\item There is no convenient equivalent of integers in \bake/; I omit
the rule \rul{SB\_Int}.
\item \Bake/ does not do |let|-generalization. I thus modify
\rul{SB\_Let} and \rul{SB\_DLet} 
to use the $[[*|-sb]]$ judgment instead of the
generalizing judgment.
\item System SB skolemizes deeply in its checking $[[*|-sb]]$ judgment,
while \bake/ skolemizes only shallowly. We thus move the prenex operation
from \rul{SB\_DeepSkol} to \rul{SB\_Infer}. I claim that this change
does not alter the set of programs that System SB accepts, due to the
fact that neither non-\rul{Infer} rule in the $[[|-sb]]$ judgment interacts
with $[[forall]]$s.
\item \Bake/ expends a great deal of effort tracking telescopes of
unification variables, requiring the notion of a generalizer $[[X]]$.
However, in the language supported by System SB, all type variables always
have kind $[[Type]]$ and so these telescopes are unnecessary. We thus
simply ignore generalizers and the generalization judgment (which always
succeeds, regardless).
\end{itemize}

The theorem below also needs to relate a context $[[P]]$ used in \bake/ with
the more traditional context $[[G]]$ used in System SB. In the claim below,
I use $[[P]] \approx [[G]]$ to mean that all $[[P]]$ has no coercion bindings,
that all irrelevant bindings in $[[P]]$ are of kind $[[Type]]$, and that
no relevant bindings depend on any other. Furthermore, all unification
variables bound in $[[P]]$ are absent from $[[G]]$.

I can now make the following claim:
\begin{claim*}[Conservativity with respect to System SB {[\pref{claim:sb}]}]
Assume $[[P]] \approx [[G]]$.
\begin{enumerate}
\item If $[[G |-sb _t => k]]$, then
$[[S;P |->ty _t ~> \cdot : k -| O]]$.
\item If $[[G *|-sb _t => k]]$, then
$[[S;P *|->ty _t ~> \cdot : k -| O]]$.
\item If $[[G |-sb _t <= k]]$, then
$[[S;P |->ty _t : k ~> \cdot -| O]]$.
\item If $[[G *|-sb _t <= k]]$, then
$[[S;P *|->ty _t : k ~> \cdot -| O]]$.
\end{enumerate}
\end{claim*}
A detailed argument for this claim appears in \pref{sec:app-sb}.

\section{Practicalities}

I have designed \bake/ with an eye toward implementing this algorithm directly
in GHC. This section discusses some of the practical opportunities and challenges
in integrating \bake/ with the rest of GHC/Haskell.

\subsection{Class constraints}

In both \pico/ and \bake/, I conspicuously ignore the possibility of Haskell's
type classes and instances. However, this is because classes and instances
are already subsumed by these formalizations' handling of regular variables.

Classes in Haskell are already compiled into record types that store the
implementations of methods, and instances
are record values (often called \emph{dictionaries}) (\pref{sec:type-classes}).
As \pico/ supports datatypes, it also supports classes. Nothing about
the type class system should matter at all in \pico/. Indeed, System FC
as currently implemented in GHC 8 cares not about type classes, to no ill
effect.

During type inference, on the other hand, we need to care a bit about classes
and instances, because these are values that the type inference mechanism
fills in for us. However, with \bake/'s ability to distinguish visible
arguments from invisible ones and its orthogonal ability to work with
variables of different relevances, the answer is right in front of us:
an instance is simply an inferred, relevant argument. That's it! These
are handled in the following rule, part of the judgment that converts a
user-written polytype into \pico/:
\[
\ottdruleIPtCXXConstrained{}
\]
This rule checks the constraint $[[_t]]$, making sure it is well-typed
as a constraint (see \pref{sec:constraint-vs-type}) and then checks
the rest of the type, assuming the constraint. The use of a $\$$ sign
in the name of the constraint ($[[dollar a]]$) is meant to convey that
the variable $[[dollar a]]$ cannot appear in the Haskell source.

Note that ``given'' class constraints (that is, a user-written context on
a function type signature) are also handled without any effort, as a member
of a telescope that unification variables are quantified over.

In contrast to the \bake/ constraint generation algorithm, the \emph{solver}
must treat instances separately and have a way of finding instances in the
global set. However, this remains out of scope for this dissertation.

\subsection{Scoped type variables}

%{
%if style == poly
%format example1
%format example2
%format ttt = "[[_t]]"
%else
\begin{code}
ttt :: forall a. a -> a
ttt _ = undefined
higherRank _ = undefined
\end{code}
%endif
Scoped type variables in GHC/Haskell have an idiosyncratic set of rules
detailing when variables are to be brought into scope~\cite{scoped-type-variables}. Consider the following two examples, where |ttt| is an arbitrary term:
\begin{code}
example1 = (ttt :: forall a. a -> a)

higherRank :: (forall a. a -> a) -> ()
example2 = higherRank ttt
\end{code}
In |example1|, the type variable |a| is in scope in |ttt|. In |example2|,
however, |a| is not in scope. This is true despite the fact that, in both
cases, \bake/ would check |ttt| against the same \pico/ type.

Instead of trying to track all of this in the constraint generation algorithm,
however, \bake/ divides its pool of variable names into those names that
can appear in a source program ($[[a, b, x]]$) and those that cannot
($[[dollar a, dollar b, dollar x]]$). When \bake/ must put a variable in the
context that should not be available in Haskell, it uses the $[[dollar a]]$
variant. Scoped type variables are explicitly brought into scope by
$\lambda$ or $\Lambda$. It is thus up to the preprocessor which must introduce
abstractions as necessary to bring the scoped type variables into scope;
as this process is not type-directed, incorporated this into the
preprocessor should not be a challenge.
%}

\subsection{Correspondence between \bake/ and GHC}

\begin{table}
\begin{center}
\begin{tabular}{cl}
\bake/ judgment & GHC function \\ \hline
$[[|->fun]]$ & |matchExpectedFunTys| \\
$[[|->scrut]]$ & |matchExpectedTyConApp| \\
$[[|->inst]]$ & |topInstantiate| \\
$[[|->pre]]$ & |tcDeepSplitSigmaTy_maybe| \\
$[[*<=]]$ & |tcSubTypeDS| \\
$[[<=]]$ & |tcSubType| \\
$[[|->prog]]$ & |tcPolyBinds|
\end{tabular}
\end{center}
\caption{GHC functions that already implement \bake/ judgments}
\label{tab:bake-ghc}
\end{table}

The design of \bake/ is already quite close to that of GHC's constraint-generation
algorithm. \pref{tab:bake-ghc} lists correspondences between \bake/ judgments
and functions already existent in GHC.

Notably absent for \pref{tab:bake-ghc} are the main judgments such as
$[[|->ty]]$. These are implemented in GHC via its |tcExpr| function,
which handles both directions of the bidirectional type system at the same
time through its use of \emph{expected types}, a mechanism there the
synthesis judgment is implemented by checking against a \emph{hole}---essentially,
a unification variable that can unify with a polytype. A full accounting
of GHC's expected types and holes is out of scope here, but there should be
no trouble adapting \bake/'s bidirectional algorithm to GHC as previous
bidirectional algorithms have been adapted.

\subsection{Unification variables in GHC}
\label{sec:ghc-vars-have-kinds}

The GHC implementation takes a very different approach to unification variables
and zonking than does \bake/. A GHC unification variable (called a metavariable
in the source code) is a mutable cell. The solver fills in the mutable cells.
Though the implementation details differ a bit, the same is currently true
to unification coercion variables (called coercion holes in GHC)---they are still
mutable cells. The \emph{zonking} operation walks through a type (or coercion
or expression) and replaces pointers to mutable cells with the cells' contents.

Another key difference between GHC and my formalization (and every other) is
that GHC variables track their own kinds. The implementation does track a
context used in looking up user-written variable occurrences, but no context
is needed to, say, extract a type's kind from the type itself. Because of
this design, GHC does not need to track unification telescopes, even though
GHC 8 already can have arbitrarily long chains of variables that depend
on others. Instead, the solver takes (essentially) the set of unification
variables to solve for. Dependency checking is done after the fact as a simple
pass making sure all variables in kinds are in scope.

A further consequence of GHC's design is that there is no need for
the concept of generalizers $[[X]]$ as I have described. Unification
variable occurrences are not, in fact, applied to vectors. Along with
the fact that GHC does not track contexts, it also uses stable names powered
by a enumerable collection of |Unique|s. We thus do not have to worry
about arbitrary $\alpha$-renaming during constraint generation and solving.
Taken together, the need for generalizers is lost, and thus the generalization
operation $[[+>]]$ disappears.

\subsection{|Constraint| vs.~|Type|}
\label{sec:constraint-vs-type}
Haskell includes the kind |Constraint| that classifies all constraints;
we thus have |Show :: Type -> Constraint|. However, due to the datatype
encoding of classes and the dictionary
encoding of instances,
\pico/ manipulates constraints just as it does
ordinary types. For this reason, \pico/ makes no distinction between
|Constraint| and |Type|. This choice follows GHC's current practice, where
|Constraint| and |Type| are distinct in the source language but indistinguishable
in the intermediate language. This design has some unfortunate consequences;
see GHC ticket \href{https://ghc.haskell.org/trac/ghc/ticket/11715}{\#11715} for a considerable
amount of discussion.

Extending the language with dependent types is orthogonal to the problems
presented there, however. For simplicity, \bake/ as described here does
not recognize |Constraint|, putting all constraints in the kind |Type| with
all the other types. 

\section{Discussion}

\subsection{Further desirable properties of the solver}
\label{sec:solver-properties}

\begin{figure}
\begin{property}[Solver is guess-free]
\label{prop:solver-guess-free}
If $[[S;P |->solv O ~> D; Z]]$, then $[[S;P,O ||= D, E_ Z]]$,
where $[[E_ Z]] = \{ [[_: au_{} [k]~[k] t]] \mathrel{\pipe} [[forall zs. t/au \in Z]] \}$
is the equational constraint induced by the zonker $[[Z]]$.
\end{property}
The above property is adapted from \citet[Definition 3.2 (P1)]{outsidein}.

\begin{property}[Solver avoids non-simple types]
\label{prop:solver-tau}
If $[[S;P |->solv O ~> D; Z]]$ and $[[forall zs. t/au \in Z]]$, then
$[[t]]$ is a simple type, with no invisible binders (at any level of structure)
and no dependency.
\end{property}

\begin{property}[Solver does not generalize over coercions]
\label{prop:solver-no-co-abs}
If $[[S;P |->solv O ~> D; Z]]$, then $[[D]]$ binds no coercion variables.
\end{property}
\caption{Addition solver properties}
\label{fig:extra-solver-properties}
\end{figure}

Thus far, I have stated only one property (in \pref{sec:solver-soundness})
that the solver must maintain, that it must output a valid zonker. However,
it is helpful to describe further properties of the solver in order to make
type inference more predictable and to maintain the properties stated by
\citet{outsidein}, such as the fact that all inferred types are principal and
that the solver makes no guesses. The full set of extra properties are listed
in \pref{fig:extra-solver-properties}.

\subsubsection{Entailment}

\begin{table}
\setlength{\mathindent}{0pt}
\newcommand{\secondline}[1]{\multicolumn{1}{r}{#1}}
\begin{center}
\begin{tabular}{@@{}llr@@{}}
Reflexivity & $[[S;P,D ||= D]]$ & (R1) \\
Transitivity & $[[S;P,D1 ||= D2]] \wedge [[S;P,D2 ||= D3]] \implies [[S;P,D1 ||= D3]]$ & (R2) \\
Substitution & $[[S;P,D1,P' ||= D2]] \wedge [[S;P |=subst theta : D1]]$ & (R3) \\
& \secondline{$\implies [[S;P,P'[theta] ||= D2[theta] ]]$} \\[1ex]
Type eq.~reflexivity & $[[S;P |=ty t : k]] \implies [[S;P ||= _: t [k]~[k] t]]$ & (R4) \\
Type eq.~symmetry & $[[S;P ||= _: t1 [k1]~[k2] t2]] \implies [[S;P ||= _: t2 [k2]~[k1] t1]]$ & (R5) \\
Type eq.~transitivity & $[[S;P ||= _: t1 [k1]~[k2] t2]] \wedge [[S;P ||= _: t2 [k2]~[k3] t3]]$ & (R6) \\
& \secondline{$ \implies [[S;P ||= _: t1 [k1]~[k3] t3]]$} \\
Conjunctions & $[[S;P ||= D1]] \wedge [[S;P ||= D2]] \implies [[S;P ||= D1,D2]]$ & (R7) \\
Substitutivity & $[[S;P ||= _: t1 [k0]~[k0] t2]] \wedge [[S;P,a :Rel k0 |=ty t : k]]$ & (R8a) \\
& \secondline{$ \implies [[S;P ||= _: t[t1/a] [k]~[k] t[t2/a] ]]$} \\
& $[[S;Rel(P) ||= _: t1 [k0]~[k0] t2]] \wedge [[S;P,a :Irrel k0 |=ty t : k]]$ & (R8b) \\
& \secondline{$ \implies [[S;P ||= _: t[t1/a] [k]~[k] t[t2/a] ]]$}
\end{tabular}
\end{center}
\caption{Required properties of entailment, following \citet[Figure 3]{outsidein}}
\label{tab:entailment}
\end{table}

These properties are stated with respect to an entailment relation, defined
as follows:
\begin{definition*}[Entailment]
We say that an environment $[[S]];[[P]]$ entails a telescope $[[D]]$,
written $[[S;P ||= D]]$, if there exists a vector $[[ps]]$ such that
$[[S;P |=vec ps : D]]$.
\end{definition*}
As this section expands upon the ideas from \citet{outsidein}, it is necessary
to check whether this definition of entailment satisfies the entailment
requirements from that work. These requirements are presented in
\pref{tab:entailment}.

All of this properties are easily satisfied, except for property (R8) (both
components) which requires congruence. As explored in some depth
in \pref{sec:congruence}, \pico/ simply does not have this property. However,
that same section argues that equality in \pico/ is ``almost congruent'',
suggesting that the equality relation truly is congruent in the absence of
coercion abstractions. The proof that the \outsidein/ algorithm infers
principal types does require property R8 \cite[Theorem 3.2]{outsidein}, and so it is possible that
\pico/'s lack of congruence prevents \bake/ from inferring principal types.
The details have yet to be worked out.

\subsubsection{A guess-free solver}

One of the guiding principles I set forth at the beginning of this chapter
is that the algorithm and solver be guess-free. We thus must assert that
the solver is guess-free, an important step along the way to the proof
of principal types in \citet{outsidein}. See \pref{prop:solver-guess-free}.

\subsubsection{Solver does not introduce impredicativity}
\label{sec:solver-properties-impredicativity}

An important but previously unstated property is that that solver must
not set a unification variable to anything but a simple type, one with
no invisible binders nor dependency. (Such types are sometimes called
$\tau$-types, referring to the $\tau/\sigma$ split in the typical presentation
of the Hindley-Milner type system.) In the context of Dependent Haskell,
impredicativity has perhaps an unusual definition: no type variable is
ever instantiated with a non-simple type. For this to hold, however, we
must make sure that this property extends to unification variables as well,
as those are sometimes used to instantiate regular variables.

Solving unification variables with simple types is also important in the
context of the theory around principal types developed in my prior
work~\cite{visible-type-application}. Specifically, we must ensure that
there are no invisible binders that are hidden underneath a unification
variable. By forbidding filling a unification variable with a non-simple
type, we have achieved this goal. See \pref{prop:solver-tau}.

\subsection{No coercion abstractions}
\label{sec:no-coercion-abstractions}

In stating that \pico/ supports type erasure (\pref{sec:type-erasure}), I admit that type erasure does not mean that we can erase coercion abstractions or applications, even though we can erase the coercions themselves.
Nevertheless, I argue that \pico/ can claim to support full type erasure
because \bake/ never produces a \pico/ program that evaluates to a coercion
abstraction. To support this claim, we can look at the elaborated program
produced by \bake/ and where coercion abstractions can be inserted:

\begin{description}
\item[Around subsumption:] Three rules extract out a telescope of binders
using the $[[|->pre]]$ judgment and then use these binders in the elaboration.
If the telescope includes a coercion binder, the elaboration will include
a coercion abstraction. However, I am arguing that there should be no
coercion binders there in the first place, so we can handle this case
essentially by induction. (Rules affected: \rul{ITyC\_Infer}, rules in the
$[[|->pre]]$ judgment, and \rul{ISub\_DeepSkol})

\item[During generalization after running the solver:] If the solver produces
a telescope that binds coercions, \bake/ will similarly include a coercion
abstract in its elaboration. We must thus assert \pref{prop:solver-no-co-abs}.
This property is not as restrictive as it may seem, as the solver may still
abstract over a class constraint whose instances store a coercion.\footnote{For example, the Haskell equality constraint |~| is such a class, distinct from
the primitive equality operator in \pico/. In the terminology of
\citet{deferred-type-errors}, the Haskell equality is lifted while the
\pico/ equality is unlifted.} (Rule affected: \rul{IDecl\_Synthesize})

\item[Elaborating |case| alternatives:] When elaborating a |case|
alternative, coercion abstractions are inserted. This is necessary for
two reasons:
\begin{itemize}
\item GADT equalities can be brought into scope in a |case| alternative.
These are bound by coercion abstractions.
\item The dependent-pattern-match equality (\pref{sec:dependent-pattern-match})
must be brought into scope by a coercion abstraction.
\end{itemize}
However, when a |case| expression is evaluated
(by evaluation rule \rul{S\_Match}), these coercion abstractions will be
applied to arguments and thus cannot be the final value of evaluating the
overall \pico/ expression. (Rules affected: \rul{IAlt\_Con}, \rul{IAltC\_Con})
\end{description}

These are the only \bake/ rules that can include a coercion abstraction in
their elaborated types. I thus conclude that type erasure is valid, with
no possibility of having evaluation be stuck on a coercion abstraction.

\subsection{Comparison to \citet{gundry-thesis}}
\label{sec:gundry-type-inference}

The \bake/ algorithm presented here is very similar to the type inference
algorithm presented by \citet[Chapter 7]{gundry-thesis}. Here I review
some of the salient differences.

\begin{itemize}
\item Gundry includes both a non-deterministic elaboration process and
a deterministic one, proving that the deterministic process is sound
with respect to the non-deterministic process (at least, in the absence
of |case|). I have omitted a non-deterministic version of the algorithm,
instead using the soundness of the resultant \pico/ program to set an upper
limit on the programs that \bake/ can accept.

\item Gundry's \emph{inch} source language and his \emph{evidence}
intermediate language have two forms of |case| statement: one for
traditional, non-dependent pattern matching; and one for dependent
pattern matching. \bake/ chooses between these possibilities using
the difference between checking and synthesis modes.

\item While Gundry uses two separate judgments in synthesis mode,
he uses only one checking judgment. The need for two judgments here
is an innovation that derives from the need for principal types, as
explored in my prior work~\cite{visible-type-application}.

\item The \emph{inch} language does not allow annotations on the
binders of a $\lambda$-abstraction and so Gundry did not encounter the
thorny case detailed in \pref{sec:annotated-lambdas}.

\item Gundry's approach to delayed instantiation for function arguments
follows along the lines of \citet{simple-bidirectional}, using an
auxiliary judgment to control function application. While \bake/ has its
$[[*|->arg]]$ judgment, which is superficially similar, \bake/'s judgment
can only handle one argument at a time.

\item Gundry's algorithm does not do deep skolemization. It would thus
not be backward compatible with GHC's current treatment of higher-rank
types.

\item Gundry gives more details about the solver in his
  algorithm~\cite[Section 7.5.1]{gundry-thesis}. However, this solver
is a novel algorithm that remains to be implemented. Instead, \bake/
targets the \outsidein/ solver. Nevertheless, I do not think it would
be hard for Gundry's general approach to target \outsidein/, as well.

\item As a point of similarity, Gundry's and \bake/'s treatment of
unification variables are very closely aligned. This is not actually
intentional---after reading Gundry's approach, I believed I could make
the whole treatment of unification variables much simpler. Yet despite
a variety of attempts, I was unable to make the basic lemmas that hold
together a type system (e.g., substitution, regularity) go through without
something as ornate as we have both used. I would love to see a simpler
treatment in the future, but I do not hold out much hope.
\end{itemize}

\section{Conclusion}

This chapter has presented \bake/, a type checking / inference / elaboration
algorithm that converts type-correct Dependent Haskell types and expressions
into \pico/. It is proven to produce type-correct \pico/ code, and it is
designed in the hope of supporting principal types. Formulating a statement
and proof of principal types in \bake/ is important future work.

This algorithm is also designed to work well with GHC's existing type checker
infrastructure, and in particular, its constraint solver. It is my hope and
plan to implement this algorithm, quite closely to how it is stated here,
in GHC in the near future.

%%  LocalWords:  newcode isNothing endif forall SB GHC GHC's zonker dcomp Int
%%  LocalWords:  applySing matchability letrec AbsBinds Zonking zonking zonk
%%  LocalWords:  Zonkers touchability Bool GADT zonkers Gundry's Hindley FC
%%  LocalWords:  isShorterThan Milner parameterisation wanteds higherRank ttt
%%  LocalWords:  matchExpectedFunTys matchExpectedTyConApp topInstantiate
%%  LocalWords:  tcDeepSplitSigmaTy tcSubTypeDS tcSubType tcPolyBinds tcExpr
%%  LocalWords:  Gundry
